{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGV6MQNvuNKU"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/arangodb/interactive_tutorials/blob/master/notebooks/Graph_Retail_EDA_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQEKKMNyuNKX"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone -b retail_data_branch https://github.com/arangodb/interactive_tutorials/ retail_data\n",
    "!rsync -av retail_data/notebooks/data ./ --exclude=.git\n",
    "!rsync -av retail_data/notebooks/tools ./ --exclude=.git\n",
    "!chmod -R 755 ./tools\n",
    "\n",
    "!pip install python-arango\n",
    "!pip install arangopipe==0.0.70.0.0\n",
    "!pip install pandas PyYAML==5.1.1 sklearn2\n",
    "!pip install jsonpickle\n",
    "!pip install -U yellowbrick\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fXKt0RXHuNKY",
    "outputId": "d1bfb06e-b6bf-47e7-d142-bfe4f39003eb"
   },
   "outputs": [],
   "source": [
    "! wc -l data/user_item_utility_matrix.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bvGXERmuNKY"
   },
   "source": [
    "# Overview\n",
    "This notebook is the second notebook of the series that applies graph analytics to extract insights from a real world retail dataset. The previous notebook performed exploratory data analysis and created a user-item-utility representation of the frequent customers to store. This data representation of the store customer is inherently high dimensional. In this notebook, a feature selection method is applied to reduce the dimensionality of the customer representation. The __RFM__ concept discussed briefly in the previous notebook, discussed in more detail here, is used to perform feature selection. The details are provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOi9dh69uNKZ"
   },
   "source": [
    "   ## Customer Value using Recency-Frequency-Monetary(RFM) value metric "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWk-Di_BuNKZ"
   },
   "source": [
    "The RFM idea is commonly used in retail to perform [customer segmentation](https://thegood.com/insights/rfm-analysis-convert/).In this work, we will use the RFM value for feature selection. To begin with, rather than consider all three attributes, we will compute an RFM score as a weighted combination of the customers recency, frequency and monetary value attributes. The $RFM\\_score$ for a customer, $c$, is computed as:\n",
    "\\begin{equation*}\n",
    "RFM\\_score_{c} = weight_{frequency} . frequency_c + weight_{recency}. frequency_c + weight_{monetary value} monetary\\_value_c\n",
    "\\label{eq:rfm_score_full_defn} \\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "Recall that the customer, $c$, is represented in terms of the items he or she has purchased at the store. There are $2505$ items in the inventory purchased by the frequent shopper group. The purpose of this notebook is to apply a feature selection algorithm to reduce this high dimensional representation to a more tractable one. We will illustrate that most of the customer value in the store is associated with a small set of store inventory items. We will posit a linear relationship between the $RFM\\_score$ for a customer and their store purchases,i.e., \n",
    " \n",
    "\\begin{equation*}\n",
    "  RFM\\_score_c = \\sum_{i=1}^{i=d} s_{ci} \\beta_i\n",
    "\\label{eq:sum_lasso} \\tag{2}\n",
    "\\end{equation*}\n",
    "\n",
    "Here $s_{ci}$ indicates the amount spent by customer, $c$, on item, $i$, at the store. The $\\beta_i$ are model coefficients.  Later in this work, we will evaluate if assuming such a linear model between the $RFM\\_score$ and the customer purchases is reasonable. In this work, we will add other constraints to this linear relationship. In particular, we will use an idea called _regularization_ with the model. In a conventional linear model, the coefficients, $\\beta_i$, are determined by minimizing the error between the model estimates and the actual observed value. In a _regularized_ model, an additional constraint is placed, we want to make these coefficients as small as possible, driving them to a zero value, if possible. The __Least Angle Shrinkage Selector__ [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics)) is a popular and effective linear model with this regularization feature. The interested reader may consult [Statistical Learning with Sparsity](https://web.stanford.edu/~hastie/StatLearnSparsity_files/SLS_corrected_1.4.16.pdf) for the details of the LASSO . Expressed as matrices, the model we are using here is:\n",
    "\n",
    "\\begin{equation*}\n",
    " RFM\\_score = \\mathbf{S}.\\mathbf{\\beta}\n",
    "\\label{eq:matrix_lasso} \\tag{3}\n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    "\n",
    "$RFM\\_score$, is a $N \\times 1$ matrix of computed RFM scores,\n",
    "\n",
    "$\\mathbf{S}$, is a $N \\times d$ matrix of customer spend on items,\n",
    "\n",
    "$\\mathbf{\\beta}$, is a $d \\times 1$ matrix of model coefficients.\n",
    "\n",
    "The LASSO minimizes the following objective to determine the best linear model:\n",
    "\\begin{equation*}\n",
    " min_{\\beta \\in \\mathcal{R}^d} \\left\\{\\frac{1}{N} \\left\\| RFM\\_score - \\mathbf{s}\\mathbf{\\beta} \\right\\|^2  + \\lambda \\left\\|\\mathbf{\\beta}\\right\\|_1 \\right\\} \n",
    "\\label{eq:obj_lasso} \\tag{4}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "There are $2505$ inventory items for this group and there are $618$ customers in the frequent shopper group. So $d=2505,\\ N= 618$, in the above equation. An examination of the above objective shows that applying LASSO to estimate a linear model requires the selection of the regularization parameter $\\lambda$. Once this parameter is selected, the LASSO solution can be computed. After computing the LASSO solution, the model coefficients, $\\beta$ are known. Since we are using LASSO for feature selection, we need to determine how many features we want to select from the model. As we will illustrate, a small proportion of these coefficients account for most of the customer value. We will use an experiment to determine the optimal number of features to extract for this dataset. We will use cross-validation to determine the optimal value of the regularization parameter, $\\lambda$. We will then evaluate if our assumption about using a linear model for customer value, for the purposes of feature selection, is a reasonable one. Before the linear model can be applied to the data, there is an important pre-processing step. An evaluation of the distribution of the $RFM\\_score$ shows a skewed, non-normal distribution. Linear models work well when the response is _Normally_ distributed. _Since $RFM\\_score$ is an engineered attribute, we might as well take an additional step and transform it to form that is known to be a desirable characteristic_. This additional step is the transformation of the $RFM\\_score$ that makes it normally distributed. A [__Box-Cox__](https://en.wikipedia.org/wiki/Power_transform) transoformation is used to transform the $RFM\\_score$ to one that has a normal distribution. \n",
    "\n",
    " The workflow for this notebook is shown below ![workflow](https://github.com/rajivsam/interactive_tutorials/blob/master/notebooks/img/feature_selection_overview.png?raw=1)\n",
    "\n",
    "The rest of this notebook, provides an implementation of this workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgB3ZuIVuNKd"
   },
   "source": [
    "## Read the User-Item-Utility-Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwpBYOMzuNKg"
   },
   "source": [
    "The input data file for this notebook is the user-item-utility matrix that was created at the end of the notebook implementing the first phase of this series. The following block, reads the matrix and identifies the data blocks corresponding to the predictor variables, the $\\mathbf{S}$ matrix and the raw __RFM__ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfRwjjgFuNKg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "fp = \"data/user_item_utility_matrix.csv\"\n",
    "df = pd.read_csv(fp)\n",
    "df_u = df[\"Customer ID\"]\n",
    "col_y = [\"MonetaryValue\", \"freq\", \"recency\"] \n",
    "cols = df.columns.tolist()\n",
    "col_exclude = [\"MonetaryValue\", \"freq\", \"recency\", \"Customer ID\"]\n",
    "col_x = [ c for c in cols if c not in col_exclude]\n",
    "df_y = df[col_y]\n",
    "df_x = df[col_x] # This is the S matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_zP9BLouNKh"
   },
   "source": [
    "### Standardize the Spend Matrix \n",
    "The columns of the customer spend matrix are standardized. This removes the mean and transforms all variables to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_w7ACEFuNKh"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "x = df_y.values #returns a numpy array\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "df_y = pd.DataFrame(x_scaled)\n",
    "df_y.columns = col_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REYl_nq_uNKi"
   },
   "source": [
    "### Compute the $RFM\\_score$ Attribute\n",
    "For this exercise, a weight of $0.7$ is used for monetary value, a weight of $0.15$ was used for recency and frequency. This is user specified. Equation (1) is used to calculate the $RFM\\_score$ attribute. For this exercise, the monetary value is primary consideration in determining the __value__ of the customer. The computed $RFM\\_score$ is then plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "FjVg_Nn1uNKi",
    "outputId": "7f6cf65f-2eb3-4e6b-b07f-074da94edb9e"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "weights = {'wt_mv':0.7, 'wt_freq': 0.15, 'wt_rec': 0.15}\n",
    "def assign_raw_RFM_score(row):\n",
    "    score = weights['wt_mv']*row[\"MonetaryValue\"] + \\\n",
    "    weights[\"wt_freq\"] * row[\"freq\"] + weights[\"wt_rec\"]*  row[\"recency\"]\n",
    "    return score\n",
    "df_y['raw_RFM_score'] = df_y.apply(func = assign_raw_RFM_score, axis = 1)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [11, 8]\n",
    "sns.set(color_codes=True)\n",
    "sns.kdeplot(df_y[\"raw_RFM_score\"], shade = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89OkMr8SuNKi"
   },
   "source": [
    "### Observation\n",
    "Observe that the distribution of $RFM\\_score$ attribute is _skewed_ and does not resemble the _bell_ shaped curve of the _Normal_ distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhnFQ9IjuNKj"
   },
   "source": [
    "### Summarize the RFM attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "RZREM_xJuNKj",
    "outputId": "119483e2-683f-49e9-a386-4d57daa57726"
   },
   "outputs": [],
   "source": [
    "df_y = df[col_y]\n",
    "df_y.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc17kbhiuNKj"
   },
   "source": [
    "### Apply the Box-Cox Transformation to $RFM\\_score$\n",
    "\n",
    "Apply the Box-Cox transformation to the $RFM\\_score$ attribute and plot the distribution of the transformed attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6aWsRVDuNKj"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import power_transform\n",
    "trans_y = power_transform(df_y, method='box-cox', standardize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-GaFlymuNKj"
   },
   "outputs": [],
   "source": [
    "df_trans_y = pd.DataFrame(trans_y)\n",
    "df_trans_y.columns = [\"bc_MV\", \"bc_freq\", \"bc_recency\"]\n",
    "weights = {'wt_mv':0.7, 'wt_freq': 0.15, 'wt_rec': 0.15}\n",
    "def assign_RFM_score(row):\n",
    "    score = weights['wt_mv']*row[\"bc_MV\"] + \\\n",
    "    weights[\"wt_freq\"] * row[\"bc_freq\"] + weights[\"wt_rec\"]*  row[\"bc_recency\"]\n",
    "    return score\n",
    "df_trans_y['RFM_score'] = df_trans_y.apply(func = assign_RFM_score, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "uVQxaLAuuNKk",
    "outputId": "a3465bda-d079-4848-b864-1d914890f404"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [11, 8]\n",
    "sns.set(color_codes=True)\n",
    "sns.kdeplot(df_trans_y[\"RFM_score\"], shade = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QwOP6nSuNKk"
   },
   "source": [
    "### Observation\n",
    "The distribution of the Box-Cox transformed $RFM\\_score$ looks the familiar bell shaped curve of the _Normal_ distribution. A standard way to verify if a variable has a particular distribution is to plot the quantiles of the variable versus the quantiles of the distribution we are comparing against (with the mean and variance of the variable that we want to compare, here it is the $RFM\\_score$). If the variable has the behavior of the target distribution, then the quantile-qauntile plot will be a straight line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_0Spn3guNKk"
   },
   "source": [
    "### Probability plot of transformed $RFM\\_score$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "KcRiCy6SuNKl",
    "outputId": "19abec10-915f-49bb-f79b-f585691cd437"
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "stats.probplot(df_trans_y[\"RFM_score\"], dist=\"norm\", plot = plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsB1SAwjuNKl"
   },
   "source": [
    "### Create Test and Training datasets\n",
    "Generalization is a key consideration in determining the appropriate number of features to select. To evaluate generalization, we need a test set. Accordingly, two-thirds of the dataset is used for training and one-third is set aside to evaluate generalization. The following block creates the test and training datasets for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9eWmm5QuNKl"
   },
   "outputs": [],
   "source": [
    "X = df_x.values\n",
    "Y = df_trans_y['RFM_score'].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "TRNG_SIZE = X_train.shape[0]\n",
    "TEST_SIZE = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaLKtiTxuNKl"
   },
   "source": [
    "### Determine the best value of $\\lambda$, the regularization parameter for the LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRzbhhRFuNKl"
   },
   "source": [
    "The [yellow brick](https://www.scikit-yb.org/en/latest/) library is used to determine the best value of the regularization parameter $\\lambda$, see Equation(4). The [scikit-learn](https://scikit-learn.org/)implementation of the LASSO algorithm is used in this work. The $\\lambda$ selection is performed using the training dataset we created in the previous step. [Cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) is used to select the best value of $\\lambda$ for this dataset. The LASSO implementation minimizes the objective discussed in the overview section, Equation (4), and drives the model coefficients, $\\mathbf{\\beta}$, to zero if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "id": "jT7g3s89uNKl",
    "outputId": "16d20d2d-f1bc-4b23-9ebc-5f64cdabcd62"
   },
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import AlphaSelection\n",
    "from sklearn.linear_model import LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "Xexp = X_train\n",
    "Yexp = Y_train\n",
    "Xtexp = X_test\n",
    "Ytexp = Y_test\n",
    "alphas = np.arange(0.01, 0.4, 0.005)\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#reg_exp = ElasticNetCV(alphas = alphas, cv = kfold, max_iter=10000)\n",
    "reg_exp = LassoCV(alphas = alphas, cv = kfold, max_iter=10000)\n",
    "\n",
    "reg_exp.fit(Xexp, Yexp.ravel())\n",
    "exp_score = reg_exp.score(Xexp, Yexp)\n",
    "y_trng_pred = reg_exp.predict(Xexp)\n",
    "y_test_pred = reg_exp.predict(Xtexp)\n",
    "trng_fit_score = \"%0.3f\"%reg_exp.score(Xexp, Yexp)\n",
    "test_fit_score = \"%0.3f\"%reg_exp.score(Xtexp, Ytexp)\n",
    "e_trng_mse = \"%0.3f\"%mean_squared_error(Yexp, y_trng_pred)\n",
    "e_test_mse = \"%0.3f\"%mean_squared_error(Ytexp,y_test_pred)\n",
    "\n",
    "fes = {\"Training MSE\": e_trng_mse,\n",
    "       \"Test MSE\": e_test_mse,\n",
    "       \"Training $R^2$\": trng_fit_score,\n",
    "       \"Test $R^2$\": test_fit_score}\n",
    "df_fes = pd.DataFrame(fes.items())\n",
    "df_fes.columns = [\"Metric\", \"Value\"]\n",
    "print(df_fes)\n",
    "visualizer = AlphaSelection(reg_exp)\n",
    "visualizer.fit(Xexp, Yexp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bxsYi8QuNKm"
   },
   "source": [
    "### Observation:\n",
    "The best value of $\\lambda$ is about $0.35$. At this value, the lowest cross-validation error is observed. This value will be used with the LASSO for feature selection. The solution from the LASSO provides the model coefficients, $\\mathbf{\\beta}$. Since we are working with a linear model, the magnitude of the coefficient is a direct indication of a particular item towards explaining the customer $RFM\\_score$. So an ordering of the $\\mathbf{\\beta}$ in descending order gives the feature importance. That brings us to the question of the number of features we want to select. To determine the number of features, we design the following experiment. We order the features by sorting them in descending order. Starting with the most important feature, we develop a linear model and then observe the test set error with included features. We then add the next most imporant feature and then observe the test error. We repeat this process. We incrementally add all the features and plot the number of features versus the observed test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeboKV61uNKm"
   },
   "outputs": [],
   "source": [
    "ord_indices = reg_exp.coef_.ravel().argsort()[::-1]\n",
    "num_features = np.arange(2,125, 2)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "trng_err = []\n",
    "test_err = []\n",
    "for nf in num_features:\n",
    "    Xtrngexp = X_train[:, ord_indices[:nf]]\n",
    "    Xtestexp = X_test[:, ord_indices[:nf]]\n",
    "    Ytrngexp = Y_train\n",
    "    Ytestexp = Y_test\n",
    "    rfsm = LinearRegression()\n",
    "    rfsm.fit(Xtrngexp, Ytrngexp.ravel())\n",
    "    y_trng_pred = rfsm.predict(Xtrngexp)\n",
    "    y_test_pred = rfsm.predict(Xtestexp)\n",
    "    trngmse = round(mean_squared_error(Y_train, y_trng_pred), 3)\n",
    "    testmse = round(mean_squared_error(Y_test, y_test_pred), 3)\n",
    "    trng_err.append(trngmse)\n",
    "    test_err.append(testmse)\n",
    "    gap_ratio = float(trngmse)/float(testmse)\n",
    "    #print('for {} features, training error is {}, test error is {}'.format(nf, trngmse, testmse))\n",
    "    #print('for {} features, gap_ratio is {:.3f}, training error is {}, test err is {}'.format(nf, gap_ratio, trngmse, testmse))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_O-LBzEuNKm"
   },
   "source": [
    "## Observation\n",
    "1. The first $50$ or so features explain most of the variablity in the $RFM\\_score$.\n",
    "2. Increasing the number of features beyond $75$ features does not reduce the test error. There is no benefit in adding more features to the model because this does not yield a generalizable model performance. Therefore, the optimal number of features to select for this data is $75$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "W9mjRWVLuNKm",
    "outputId": "b6f2a354-975d-4e7e-cf95-489ab9df9774"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "NUM_FEATURES_OPT = 75\n",
    "aed = {\"numfeatures\": num_features, \"trngmse\": trng_err, \"testmse\": test_err}\n",
    "df_nf_sel = pd.DataFrame(aed)\n",
    "cols_to_plot = [\"numfeatures\", \"trngmse\", \"testmse\"]\n",
    "df_plot = df_nf_sel[cols_to_plot]\n",
    "df_ep = df_plot.melt(id_vars = ['numfeatures'])\n",
    "df_ep[\"value\"] = df_ep[\"value\"].astype(np.float)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [11, 8]\n",
    "sns.set(color_codes=True)\n",
    "sns.lineplot(x='numfeatures', y='value', hue='variable', data = df_ep)\n",
    "plt.axvline(x = NUM_FEATURES_OPT, color ='red', linestyle= '--') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KwOG8h-uNKm"
   },
   "outputs": [],
   "source": [
    "ord_indices = reg_exp.coef_.ravel().argsort()[::-1]\n",
    "OPT_NUM_FEATURES = 75\n",
    "\n",
    "Xtrngexp = X_train[:, ord_indices[:OPT_NUM_FEATURES]]\n",
    "Xtestexp = X_test[:, ord_indices[:OPT_NUM_FEATURES]]\n",
    "Ytrngexp = Y_train\n",
    "Ytestexp = Y_test\n",
    "rfsm = LinearRegression()\n",
    "rfsm.fit(Xtrngexp, Ytrngexp.ravel())\n",
    "y_trng_pred = rfsm.predict(Xtrngexp)\n",
    "y_test_pred = rfsm.predict(Xtestexp)\n",
    "trngmse = round(mean_squared_error(Y_train, y_trng_pred), 3)\n",
    "testmse = round(mean_squared_error(Y_test, y_test_pred), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fdfzO_wuNKm"
   },
   "source": [
    "## Is the assumption of a linear relationship between customer spend and the transformed $RFM\\_score$ valid?\n",
    "\n",
    "To verify if the assumption of a linear relationship between the $RFM\\_score$ and the customer spend, we can plot the values predicted by such a linear model versus the true $RFM\\_score$ on the test set. If the model is reasonable, then we should expect agreement or similar values for the model prediction and the ground truth. If we plot the predicted versus the actual values, we should see a straight line. This is what we observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "0tAn2VfauNKn",
    "outputId": "bc7afebf-f74f-46ec-dedb-092aeb9d4055"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [11, 8]\n",
    "sns.set(color_codes=True)\n",
    "xp = np.linspace(Ytestexp.ravel().min(), Ytestexp.ravel().max(), 30)\n",
    "yp = xp\n",
    "g = sns.scatterplot(x=Ytestexp.ravel(), y=y_test_pred.ravel())\n",
    "plt.plot(xp, yp, color='r')\n",
    "plt.title(\"Actual vs Fitted Values (test set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXi6LhCWuNKn"
   },
   "source": [
    "### Observation:\n",
    "A plot of the predicted versus actual shows reasonable agreement. Perfect agreement is the red line. The difference between the model prediction and the actual value is the _error_ . We will evaluate the nature of this _error_ shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpl5T53CuNKn"
   },
   "outputs": [],
   "source": [
    "feats = col_x\n",
    "if \"Customer ID\" in feats:\n",
    "    feats.remove(\"Customer ID\")\n",
    "feats = np.array(feats)\n",
    "bm_ord_ind = reg_exp.coef_.ravel().argsort()[::-1]\n",
    "feats = feats.ravel()[bm_ord_ind][:NUM_FEATURES_OPT]\n",
    "bmcoefs = reg_exp.coef_.ravel()[bm_ord_ind][:NUM_FEATURES_OPT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8R0GzvC_uNKn"
   },
   "source": [
    "### Examine the Error\n",
    "A density plot of the error provides information about the distribution of its values. A _probability-probablity_ plot of the error with normal distribution tells us if it is reasonable to approximate the errors as being _normal_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJ4amemyuNKn"
   },
   "outputs": [],
   "source": [
    "test_err = y_test_pred - Ytestexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "-F6-SY8fuNKn",
    "outputId": "1b507055-9ce1-46c1-d7f3-3b96175004de"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [11, 8]\n",
    "sns.set(color_codes=True)\n",
    "sns.distplot(test_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgfErh2MuNKn"
   },
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "LxtfXH-6uNKn",
    "outputId": "3a633bfc-72ba-46c2-bae9-ecca8c24694b"
   },
   "outputs": [],
   "source": [
    "stats.probplot(test_err, dist=\"norm\", plot = plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_L3EOF9GuNKo"
   },
   "source": [
    "### Observation\n",
    "1. An evaluation of the density plot shows that we have a small number of points where the error is large.\n",
    "2. On the probability-probability plot, these points show up as those being greater than 3 standard deviations (top right corner of the plot above)\n",
    "\n",
    "These points are _outliers_. See any standard reference or resource on linear regression, for example,[this reource](https://online.stat.psu.edu/stat462/node/173/) for a more detailed understanding of outliers. The _Cook's Distance_ will be used to remove this problematic points from the dataset. We will then redo the linear model and check the distribution of errors. If the outliers have been removed, the errors from the model should be within 3 standard deviations. The _yellowbrick_ library comes with an implementation that we can use for this purpose. The details are shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gtq8rmwSuNKo"
   },
   "outputs": [],
   "source": [
    "Xfs = X[:, bm_ord_ind[:NUM_FEATURES_OPT]]\n",
    "Yfs = Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0l8wNzrvuNKo"
   },
   "source": [
    "## Remove the Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "nZQgzNRXuNKo",
    "outputId": "e1043f90-6fd3-409f-a4e7-735018066d19"
   },
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import CooksDistance\n",
    "visualizer = CooksDistance()\n",
    "visualizer.fit(Xfs, Yfs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etS20RbEuNKo"
   },
   "outputs": [],
   "source": [
    "i_less_influential = (visualizer.distance_ <= visualizer.influence_threshold_)\n",
    "X_li, Y_li = Xfs[i_less_influential], Yfs[i_less_influential]\n",
    "customers = df_u.values[i_less_influential]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZoFSQwauNKo"
   },
   "source": [
    "## Verify Model after Outlier Removal\n",
    "\n",
    "__We develop the linear model and verify it again after outlier removal__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "lfP0sz_6uNKo",
    "outputId": "9923feb3-0bc0-47df-da56-e8e649c84ea9"
   },
   "outputs": [],
   "source": [
    "X = X_li\n",
    "Y = Y_li\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "TRNG_SIZE = X_train.shape[0]\n",
    "TEST_SIZE = X_test.shape[0]\n",
    "rfsm = LinearRegression()\n",
    "rfsm.fit(X_train, Y_train.ravel())\n",
    "y_trng_pred = rfsm.predict(X_train)\n",
    "y_test_pred = rfsm.predict(X_test)\n",
    "trngmse = round(mean_squared_error(Y_train, y_trng_pred), 3)\n",
    "testmse = round(mean_squared_error(Y_test, y_test_pred), 3)\n",
    "test_err = y_test_pred - Y_test\n",
    "stats.probplot(test_err, dist=\"norm\", plot = plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "id": "dyOYVVcsuNKo",
    "outputId": "f013470e-bc53-4532-9896-4cd28e1b459f"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [11, 8]\n",
    "sns.set(color_codes=True)\n",
    "sns.distplot(test_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pljigx-nuNKp"
   },
   "source": [
    "### Observation\n",
    "_An evaluation of the errors after outlier removal shows that the errors from the model are within 3 standard deviations. So the outliers have been removed and the model has been validated!_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElGwUfdOwK1P"
   },
   "source": [
    "## Compute the variance explained by the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddJqqiSRvZuB",
    "outputId": "3bab03cb-06bd-494c-feda-c45c9c247c15"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(Y_train, y_trng_pred)\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "fRNR23a5IZDF",
    "outputId": "4cc0f65f-1741-435c-a4c4-56ffd67758c7"
   },
   "outputs": [],
   "source": [
    "key_results = {\"RFM weights\": [\"0.7, 0.15, 0.15\"],\"alpha\": [0.22],  \"R2\": [r2],  \"optimal number of features\": [OPT_NUM_FEATURES],\\\n",
    "               \"training MSE\" : [trngmse], \"test MSE\": [testmse] }\n",
    "dfkr = pd.DataFrame.from_dict(key_results).T\n",
    "dfkr = dfkr.reset_index()\n",
    "dfkr.columns = [\"FS Parameter\", \"Value\"]\n",
    "dfkr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azv_-1HLzxHQ"
   },
   "outputs": [],
   "source": [
    "feats = col_x\n",
    "NUM_FEATURES_PLOT = 50\n",
    "if \"Customer ID\" in feats:\n",
    "    feats.remove(\"Customer ID\")\n",
    "feats = np.array(feats)\n",
    "bm_ord_ind = rfsm.coef_.ravel().argsort()[::-1]\n",
    "feats = feats.ravel()[bm_ord_ind][:OPT_NUM_FEATURES]\n",
    "bmcoefs = rfsm.coef_.ravel()[bm_ord_ind][:OPT_NUM_FEATURES]\n",
    "featsp = feats[:NUM_FEATURES_PLOT]\n",
    "bmcoefsp = bmcoefs[:NUM_FEATURES_PLOT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nke1EsZKuNKp"
   },
   "source": [
    "## Plot Feature Importance for the Frequent Shopper Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "id": "gYG7g28fuNKp",
    "outputId": "987db3cb-837d-407c-d1de-fde50545f292"
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "plt.rcParams['figure.figsize'] = [24, 9.5]\n",
    "plt.rcParams['xtick.labelsize'] = 'xx-small'\n",
    "sns.set(color_codes=True)\n",
    "g = sns.barplot(x = featsp, y = bmcoefsp, ci = None)\n",
    "plt.title(\"Feature Importance from Feature Selection Step\")\n",
    "plt.grid(True)\n",
    "plt.xlabel('Item ID', fontsize=10)\n",
    "plt.ylabel('Regression Coefficient')\n",
    "plt.locator_params(axis='y', nbins=20)\n",
    "l = g.set_xticklabels(feats, rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaNIGhNc5-12"
   },
   "outputs": [],
   "source": [
    "fp = \"data/stock_code_description.csv\"\n",
    "dfsclu = pd.read_csv(fp)\n",
    "lusc = dict()\n",
    "for index, row in dfsclu.iterrows():\n",
    "    lusc[row['StockCode']] = row[\"Description\"]\n",
    "del dfsclu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "5-eQbZMq6Ky7",
    "outputId": "0b5dd40e-4d64-4bf5-8c8d-91e366af1c13"
   },
   "outputs": [],
   "source": [
    "top10desc = [lusc[feats[i]] for i in range(10)]\n",
    "dft0desc = pd.DataFrame([feats[:10], top10desc]).T\n",
    "dft0desc.columns = [\"ITEM ID\", \"Description\"]\n",
    "dft0desc.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDxLnF28uNKp"
   },
   "source": [
    "## Feature Selection Done !!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_p-Wyh08uNKp"
   },
   "source": [
    "We have now reduced the dataset to a dataset with 447 users and 75 items. The customers's purchases of items can be modeled as a bi-partite graph. In this graph, customers and the items are the vertex collections. The purchases are edges from the customer to the items and edge weight is the spend of the customer on that item. We now:\n",
    "1. Capture the meta-data about the analysis done in this notebook and record it in the Arangopipe project we created in the previous notebook. We will use the same database we used in the previous notebook.\n",
    "2. The data after feature selection can be modeled as a [__bi-partite__ graph ](https://en.wikipedia.org/wiki/Bipartite_graph#:~:text=In%20the%20mathematical%20field%20of,the%20parts%20of%20the%20graph.) with the customers and items being the two vertex sets. This __bi-partite graph__ is the graph that we have learned from the raw dataset, after feature selection. This graph is written out to ArangoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNPiE-o9uNKp"
   },
   "source": [
    "## Write the Retail Graph to ArangoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O0-W7P6euNKp",
    "outputId": "07173ed9-a691-4da2-dfc2-9358dfa34b11"
   },
   "outputs": [],
   "source": [
    "from arangopipe.arangopipe_storage.arangopipe_api import ArangoPipe\n",
    "from arangopipe.arangopipe_storage.arangopipe_admin_api import ArangoPipeAdmin\n",
    "from arangopipe.arangopipe_storage.arangopipe_config import ArangoPipeConfig\n",
    "from arangopipe.arangopipe_storage.managed_service_conn_parameters import ManagedServiceConnParam\n",
    "mdb_config = ArangoPipeConfig()\n",
    "msc = ManagedServiceConnParam()\n",
    "conn_params = { msc.DB_SERVICE_HOST : \"arangoml.arangodb.cloud\", \\\n",
    "                        msc.DB_SERVICE_END_POINT : \"createDB\",\\\n",
    "                        msc.DB_SERVICE_NAME : \"createDB\",\\\n",
    "                        msc.DB_SERVICE_PORT : 8529,\\\n",
    "                        msc.DB_CONN_PROTOCOL : 'https',\\\n",
    "                        msc.DB_REPLICATION_FACTOR: 3}\n",
    "        \n",
    "mdb_config = mdb_config.create_connection_config(conn_params)\n",
    "admin = ArangoPipeAdmin(reuse_connection = False, config = mdb_config)\n",
    "ap_config = admin.get_config()\n",
    "ap = ArangoPipe(config = ap_config)\n",
    "proj_info = {\"name\": \"Retail_Graph_Analytics\"}\n",
    "proj_reg = admin.register_project(proj_info)\n",
    "mdb_config.get_cfg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-iUV5ouquNKp"
   },
   "outputs": [],
   "source": [
    "login = mdb_config.get_cfg()['arangodb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D8cfi7j9uNKq",
    "outputId": "62748701-95d4-4001-abc6-8fabacad3253"
   },
   "outputs": [],
   "source": [
    "# Restore an ArangoDB dump into the database.\n",
    "!./tools/arangorestore  -c none --server.endpoint  http+ssl://{login[\"DB_service_host\"]}:{login[\"DB_service_port\"]} --server.username {login[\"username\"]} \\\n",
    "--server.database {login[\"dbName\"]} \\\n",
    "--server.password {login[\"password\"]} \\\n",
    "--replication-factor 3  \\\n",
    "--input-directory \"data/retail_freq_cust_data_dump_eonb1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJPIg-SbuNKq"
   },
   "outputs": [],
   "source": [
    "tut_db = admin.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYrLFBaeuNKq"
   },
   "outputs": [],
   "source": [
    "if not tut_db.has_graph(\"retail_freq_cust_graph\"):\n",
    "    tut_db.create_graph('retail_freq_cust_graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hstZL8yLuNKq"
   },
   "outputs": [],
   "source": [
    "rf = mdb_config.cfg['arangodb']['arangodb_replication_factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ROAnWb9uNKq"
   },
   "outputs": [],
   "source": [
    "rf = mdb_config.cfg['arangodb']['arangodb_replication_factor']\n",
    "retail_freq_cust_graph = tut_db.graph(\"retail_freq_cust_graph\")\n",
    "\n",
    "if not tut_db.has_collection(\"Customers\"):\n",
    "    tut_db.create_collection(\"Customers\", rf)\n",
    "if not tut_db.has_collection(\"Items\"):\n",
    "    tut_db.create_collection(\"Items\", rf)\n",
    "if not tut_db.has_collection(\"Purchases\"):\n",
    "    tut_db.create_collection(\"Purchases\", rf, edge=True)\n",
    "if not retail_freq_cust_graph.has_edge_definition(\"Purchases\"):\n",
    "    purchases = retail_freq_cust_graph.create_edge_definition(\n",
    "        edge_collection='Purchases',\n",
    "        from_vertex_collections=['Customers'],\n",
    "        to_vertex_collections=['Items']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZjN8B66uNKq",
    "outputId": "979c95bb-cd4f-4025-82e5-3f150a32f0dc"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "%time\n",
    "\n",
    "\n",
    "num_freq_cust = X.shape[0]\n",
    "batch = []\n",
    "BATCH_SIZE = 100\n",
    "batch_idx = 1\n",
    "collection =tut_db[\"Customers\"]\n",
    "\n",
    "for cidx in range(num_freq_cust):\n",
    "  pk = \"Customers/\" + str(customers[cidx])\n",
    "  insert_doc1 = {\"_id\": pk}\n",
    "  batch.append(insert_doc1)\n",
    "  \n",
    "  if len(batch) == BATCH_SIZE:\n",
    "    print(\"Inserting batch %d\" % (batch_idx))\n",
    "    collection.import_bulk(batch, on_duplicate = 'replace')\n",
    "    batch = []\n",
    "    batch_idx += 1\n",
    "  \n",
    "  last_record = (cidx == (num_freq_cust -1) )\n",
    "    \n",
    "  if last_record and len(batch) > 0:\n",
    "    print(\"Inserting batch the last batch!\")\n",
    "    collection.import_bulk(batch, on_duplicate = 'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "zNOqAXSJuNKq",
    "outputId": "ec1213b4-4a5e-47b7-d513-e3a508d76ee9"
   },
   "outputs": [],
   "source": [
    "pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-LHs1bquNKr",
    "outputId": "d41270ab-8cbf-43da-cfd3-e10224de7282"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "%time\n",
    "num_features = len(feats)\n",
    "collection = tut_db[\"Items\"]\n",
    "batch = []\n",
    "for fidx in range(num_features):\n",
    "  pk = \"Items/\" + str(feats[fidx])\n",
    "  insert_doc1 = {\"_id\": pk}\n",
    "  batch.append(insert_doc1)\n",
    "\n",
    "\n",
    "  collection.import_bulk(batch, on_duplicate = 'replace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p1k7yJb7uNKr",
    "outputId": "55629880-1478-44fa-b152-adee964fd784"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "%time\n",
    "\n",
    "\n",
    "num_freq_cust = X.shape[0]\n",
    "num_features = X.shape[1]\n",
    "batch = []\n",
    "BATCH_SIZE = 500\n",
    "batch_idx = 1\n",
    "collection =tut_db[\"Purchases\"]\n",
    "\n",
    "for cidx in range(num_freq_cust):\n",
    "    \n",
    "    \n",
    "    for fidx in range(num_features):\n",
    "        Xci = X[cidx, fidx]\n",
    "        #print(\"Processing user: %s , item %s, value: %.3f\" %(str(customers[cidx]), feats[fidx], Xci))\n",
    "        if Xci > 0:\n",
    "            fk = \"Customers/\" + str(customers[cidx])\n",
    "            tk = \"Items/\" + feats[fidx]\n",
    "            insert_doc1 = {\"_from\": fk, \"_to\": tk, \"spend\": str(Xci)}\n",
    "            batch.append(insert_doc1)\n",
    "                \n",
    "            \n",
    "            if len(batch) == BATCH_SIZE:\n",
    "                print(\"Inserting batch %d\" % (batch_idx))\n",
    "                collection.import_bulk(batch, on_duplicate = 'replace')\n",
    "                batch = []\n",
    "                batch_idx += 1\n",
    "        last_record = (cidx == (num_freq_cust -1) )\n",
    "    \n",
    "    if last_record and len(batch) > 0:\n",
    "                print(\"Inserting batch the last batch!\")\n",
    "                collection.import_bulk(batch, on_duplicate = 'replace')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPsSC_ibuNKr"
   },
   "source": [
    "## Use Arangopipe to Log Project Activity for Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQEic-iBuNKr",
    "outputId": "f72efc09-ac94-47e5-bbff-d7e6e4bb20e4"
   },
   "outputs": [],
   "source": [
    "ap_config = admin.get_config()\n",
    "ap = ArangoPipe(config = ap_config)\n",
    "# View the temporary database via the WebUI\n",
    "print(\"Run the following code to get credentials you can use at https://arangoml.arangodb.cloud:8529 to explore the graph further.\")\n",
    "print(\"The database expires after a few hours.\")\n",
    "print(\"\")\n",
    "ap.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrZEJlmOuNKr"
   },
   "source": [
    "## Lookup Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlQGUN64uNKr"
   },
   "outputs": [],
   "source": [
    "ds_info = ap.lookup_dataset(\"retail analytics dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBn0Jy0quNKs"
   },
   "source": [
    "## Lookup Featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-j5JdcT2uNKs"
   },
   "outputs": [],
   "source": [
    "fs_info = ap.lookup_featureset(\"retail analytics feature setl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9v-rEWvkuNKs"
   },
   "source": [
    "## Register Feature Selection Task with Arangopipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etKffj9SuNKs"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "rtoken = str(uuid.uuid4().int)\n",
    "model_info = {\"name\": \"Feature Selection Task\" + \"_\" + rtoken,  \"task\": \"Feature Selection\"}\n",
    "model_reg = ap.register_model(model_info, project = \"Retail_Graph_Analytics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YByyQu3QuNKs"
   },
   "source": [
    "## Log Feature Selection Task with Arangopipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1FjimaauNKs"
   },
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "import jsonpickle\n",
    "\n",
    "ruuid = str(uuid.uuid4().int)\n",
    "model_perf = {'run_id': ruuid, 'training_MSE': str(trngmse),\\\n",
    "              'test_MSE': str(testmse), \"timestamp\": str(datetime.datetime.now())}\n",
    "\n",
    "model_params = {'run_id': ruuid, 'lambda': '0.215', 'opt_features': '75'}\n",
    "\n",
    "run_info = {\"dataset\" : ds_info[\"_key\"],\\\n",
    "                    \"featureset\": fs_info[\"_key\"],\\\n",
    "                    \"run_id\": ruuid,\\\n",
    "                    \"model\": model_reg[\"_key\"],\\\n",
    "                    \"model-params\": model_params,\\\n",
    "                    \"model-perf\": model_perf,\\\n",
    "                    \"tag\": \"Retail_Graph_Analytics\",\\\n",
    "                    \"project\": \"Retail_Graph_Analytics\"}\n",
    "ap.log_run(run_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Snx-orquNKs",
    "outputId": "d4a26b84-2544-4e1f-c920-7bfc46268ac4"
   },
   "outputs": [],
   "source": [
    "\n",
    "!./tools/arangodump  -c none --server.endpoint http+ssl://{login[\"DB_service_host\"]}:{login[\"DB_service_port\"]} --server.username {login[\"username\"]} --server.database {login[\"dbName\"]} --server.password {login[\"password\"]} --overwrite true --output-directory \"data/retail_freq_cust_data_dump_eonb2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0eLJq1ecuNKs"
   },
   "outputs": [],
   "source": [
    "#Xfs = X[:, bm_ord_ind[:NUM_FEATURES_OPT]]\n",
    "Yfs = Y\n",
    "df_fs = pd.DataFrame(X)\n",
    "df_fs.columns = feats\n",
    "df_fs[\"RFM_score\"] = Y\n",
    "df_fs[\"Customer_ID\"] = df_u\n",
    "df_fs = df_fs.set_index(\"Customer_ID\")\n",
    "fp = \"data/feature_selected_freq_shoppers.csv\"\n",
    "df_fs.to_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IChXqjPbuNKs",
    "outputId": "617e59f4-3293-4e73-8c8b-a8f3e17deb41"
   },
   "outputs": [],
   "source": [
    "df_fs.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Graph_Retail_EDA_II.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
