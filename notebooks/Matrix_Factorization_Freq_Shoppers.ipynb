{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/arangodb/interactive_tutorials/blob/master/notebooks/Matrix_Factorization_Freq_Shoppers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MS35R2PjsNrA"
   },
   "source": [
    "# Graph Embeddings Using Matrix Factorization\n",
    "\n",
    "This notebook provides an overview of developing embeddings for the retail example using $\\textbf{Non-Negative Matrix Factorization}$. The perspective in this exercise is data analysis rather a specific machine learning task. The approach illustrated in this notebook provides both insights as well as embeddings. Developing the embeddings also requires fewer hyper-parameter choices than approaches such as $\\textbf{node2vec}$. The data for this exercise is the feature selected set of customer purchases of the frequent shopper group developed in [notebook2](https://github.com/arangodb/interactive_tutorials/blob/master/notebooks/Graph_Retail_EDA_II.ipynb) of the retail data analysis with ArangoDB series. The details of the anaysis are provided below.\n",
    "\n",
    "This note book produces a _knowledge graph_ that captures the preferences and shopping behavior of frequent shoppers. This _knowledge graph_ is stored in ArangoDB. This notebook develops _embeddings_ of the customers and store items. These embeddings are stored as _node properties_ of the _knowledge graph_ . The _embeddings_ can be used for similarity search and in _adhoc_ queries. Examples of performing this in _AQL_ are provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuWM8TIBg4C_"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "get_ipython().__class__.__name__ = \"ZMQInteractiveShell\"\n",
    "!git clone -b retail_data_branch --single-branch https://github.com/arangodb/interactive_tutorials.git\n",
    "!rsync -av  interactive_tutorials/notebooks/data  rsync -av interactive_tutorials/notebooks/tools interactive_tutorials/notebooks/img ./ --exclude=.git\n",
    "!pip install python-arango\n",
    "!pip install arangopipe==0.0.70.0.0\n",
    "!pip install pandas PyYAML==5.1.1 sklearn2\n",
    "!pip install jsonpickle\n",
    "!pip3 install networkx\n",
    "!pip3 install matplotlib\n",
    "!pip3 install adbnx-adapter==0.0.0.2.5.3\n",
    "!pip install hdbscan\n",
    "!pip install seaborn\n",
    "!chmod a+x tools/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUir_z0noZ08"
   },
   "source": [
    "![](/content/img/retail_freq_customer_purchases.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvV9CalUpwcY"
   },
   "source": [
    "## Read Feature Selected Frequent Shoppers\n",
    "This dataset captures the feature selected frequent shoppers after feature selection using the LASSO (see [retail data series, exploratory data analysis](https://github.com/arangodb/interactive_tutorials/blob/master/notebooks/Graph_Retail_EDA_II.ipynb) for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjdumhTTgkHj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fp = \"data/feature_selected_freq_shoppers.csv\"\n",
    "df = pd.read_csv(fp)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdxTOFqErjAU"
   },
   "source": [
    "## Bipartite Graph Representation\n",
    "This feature selected set of purchases of the frequent customers can be viewed as a bi-partite graph with the customers and the store inventory items constituting the disjoint vertex sets.\n",
    "\n",
    "![](https://github.com/arangodb/interactive_tutorials/blob/retail_data_branch/notebooks/img/retail_freq_customer_purchases.png?raw=1)\n",
    "\n",
    "\n",
    "## Overview of Embeddings from Matrix Factorization\n",
    "\n",
    "An embedding of this graph will be developed using matrix factorization. The feature selected set of purchases, $\\mathbf{P}$, is the matrix we want to factorize. This is factorized into a tall skinny matrix, $\\mathbf{W}$, and a shallow wide matrix, $\\mathbf{H}$, as shown below.\n",
    "\n",
    "![](https://github.com/arangodb/interactive_tutorials/blob/retail_data_branch/notebooks/img/matrix_factorization_algorithm_schematic.png?raw=1)\n",
    "\n",
    "The store inventory of $m$ items is grouped into $k$ purchase groups. The purchases of the $i^{th}$ customer, a vector of size $m$, is expressed as a product of the purchase groups and the customers affinities towards the purchase groups as follows:\n",
    "\n",
    "\n",
    "$$\\boxed{p_i = u_{(i,k)}. pg_{(k, m)}}$$\n",
    "\n",
    "\n",
    "\n",
    "The critical aspect of this factorization process is the determination of the latent dimension, $k$. Regularization can be factored into the matrix factorization process as discussed in the [sklearn non negative matrix factorization implementation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html). However, since we have explicitly used the LASSO with the notion of RFM score to select the features, the regularization parameters are set to zero in the matrix factorization objective function. To determine the latent dimension, $k$, a test set of purchases is held out. A range of possible values for $k$, from $2$ to $20$, were used. The mean square error for each value of $k$ is recorded. The value of $k$ that produced the lowest MSE is selected as the best value of $k$. The flowchart for the process is shown below.\n",
    "\n",
    "![](https://github.com/arangodb/interactive_tutorials/blob/retail_data_branch/notebooks/img/matrix_factorization_flowchart.png?raw=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfhs0p1bgkHm"
   },
   "outputs": [],
   "source": [
    "cust_ids = df[\"Customer_ID\"]\n",
    "rfm_scores = df[\"RFM_score\"]\n",
    "to_del = [\"Customer_ID\", \"RFM_score\"]\n",
    "cols = df.columns.tolist()\n",
    "req_cols = [c for c in cols if c not in to_del]\n",
    "df = df[req_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMyLCRbXwYpo"
   },
   "source": [
    "### Hold out $\\frac{1}{3}$ of the matrix of purchases as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j953tW8KgkHn"
   },
   "outputs": [],
   "source": [
    "TEST_PROP = 0.33\n",
    "NUM_TEST_CELLS = int(df.shape[0] * df.shape[1]* TEST_PROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bd6Hg5m-gkHn"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "dict_test = dict()\n",
    "num_test_samples = 0\n",
    "random.seed(1234)\n",
    "while num_test_samples <= NUM_TEST_CELLS:\n",
    "    row = random.randint(0, (df.shape[0] -1))\n",
    "    col = random.randint(0, (df.shape[1] - 1))\n",
    "    dict_test[(row, col)] = df.iloc[row, col]\n",
    "    df.iloc[row, col] = 0.0\n",
    "    num_test_samples += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlHebRZEgkHn"
   },
   "outputs": [],
   "source": [
    "X = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6NbjA1egkHo"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqs6aMuKwtp6"
   },
   "source": [
    "### Try a range of factors, $2$ to $20$, as possible latent dimensions and compute the mean square error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmCfqXnGgkHo"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "#np.random.seed(42)\n",
    "exp_factors = np.arange(2,21)\n",
    "recon_err = dict()\n",
    "for fac in exp_factors:\n",
    "    print(\"Computing factor %d\" % (fac))\n",
    "    model = NMF(n_components=fac, init='random', random_state=1234)\n",
    "    W = model.fit_transform(X)\n",
    "    H = model.components_\n",
    "    se = []\n",
    "    lrm = W.dot(H)\n",
    "    for k, v in dict_test.items():\n",
    "        sqerr = (lrm[k[0], k[1]] - v)**2\n",
    "        se.append(sqerr)\n",
    "    recon_err[fac] = np.sum(se)/NUM_TEST_CELLS\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLz85dHhw6uf"
   },
   "source": [
    "### Plot latent dimension versus mean square error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBiAR0AdgkHp"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [11, 8]\n",
    "sns.set(color_codes=True)\n",
    "keys = np.fromiter(recon_err.keys(), dtype=int)\n",
    "vals = np.fromiter(recon_err.values(), dtype=float)\n",
    "plt.xticks(range(1,20))\n",
    "sns.lineplot(keys, vals, marker = 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3V3J15lnxNyz"
   },
   "source": [
    "### Determine the optimal latent dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ey_lPN9qgkHp"
   },
   "outputs": [],
   "source": [
    "OPTIMAL_EMBEDDING_SIZE = 5\n",
    "n_top_words = 10\n",
    "model = NMF(n_components= OPTIMAL_EMBEDDING_SIZE, init='random', random_state=1234)\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_\n",
    "topic_features = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5vfMUqnxW1L"
   },
   "source": [
    "## Matrix Factorization\n",
    "Once the optimal latent dimension, $k$, has been identified, we can compute the matrix factorization of the purchases, $\\mathbf{P}$. This factorization provides the following approximation\n",
    "\n",
    "$$\\mathbf{P} \\approx \\mathbf{W}. \\mathbf{H}$$\n",
    "\n",
    "where:\n",
    "\n",
    "$\\mathbf{H}$: Represents the dictionary matrix. This can be interpretted as $k$ groups of store inventory items. Each of these groups is of size, $m$, the number of items in the store inventory ($75$). A single purchase group is a vector of size $m$. The magnitude of the vector elements provides an indication of the store inventory items that dominate a purchase group. As will be observed subsequently, each of these purchase groups are dominated by a small number of items. The magnitude of most of the elements of the purchase group vector are small. \n",
    "\n",
    "$\\mathbf{W}$: Represents the affinity matrix. This can be interpretted as a matrix that captures the affinities customers have towards each of the $k$ purchase groups. \n",
    "\n",
    "The elements of $\\mathbf{W}$ are the representation of the customers in the $k$ dimensional latent space. The elements of $\\mathbf{H}$ are the representations of the items in the $k$ dimensional latent space. Therefore, Matrix Factorization provides the following:\n",
    "\n",
    "\n",
    "1.   Vector representations of the customers and store inventory in a $k$ dimensional space.\n",
    "2.   An interpretation of customer purchases in terms of dictionary learning. Store inventory can be grouped into $k$ purchase groups. The purchases of custormers can be viewed in terms of their affinities towards item groups.\n",
    "\n",
    "Since $k$ is smaller than $m$, we are able to group the store inventory into a small number of groups and view customer preferences in terms of their affinities towards these groups. Therefore matrix factorization provides embeddings as well as insights into the tastes of the frequent shopper group. \n",
    "\n",
    "When embeddings for the frequent customer purchase group are developed using techniques such as node2vec, we have to specify a few more hyper-parameters to develop the embedding. For example, with node2vec, we have to specify the parameters of the random-walk, the number of walks and the length of the walk as well as the embedding size (the latent dimension $k$ in this example). Usually these are determined through a hyper-parameter optimization procedure, for example, Bayesian Optimization. In this example, we have only one hyper-parameter, the latent dimension, $k$ and this was determined through a principled experiment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmI5bGZ7gkHq"
   },
   "outputs": [],
   "source": [
    "fp = \"data/stock_code_description.csv\"\n",
    "dfsclu = pd.read_csv(fp)\n",
    "lusc = dict()\n",
    "for index, row in dfsclu.iterrows():\n",
    "    lusc[row['StockCode']] = row[\"Description\"]\n",
    "del dfsclu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anpbSt3XzuVI"
   },
   "source": [
    "## Dictionary Profile\n",
    "The top $10$ components of each of the $5$ dictionary vectors are plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6tML1hFgkHq"
   },
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "        topic_features[topic_idx] = top_features\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Item Group {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDQMiAkWgkHq"
   },
   "outputs": [],
   "source": [
    "plot_top_words(model, req_cols, n_top_words, \"MF Based Embedding of Frequent Customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dEEq_dQ0Kif"
   },
   "source": [
    "### Determine the description of the dominant components of the dictionary elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vjvP97irC75"
   },
   "outputs": [],
   "source": [
    "XH = H\n",
    "dfH = pd.DataFrame(XH)\n",
    "from sklearn import preprocessing\n",
    "\n",
    "Xh = preprocessing.normalize(dfH.values)\n",
    "dfHn = pd.DataFrame(Xh)\n",
    "dfHn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zmKn5WcsFMV"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "fig, ax = plt.subplots(figsize=(15,6)) \n",
    "igl = [\"Item Group_\" + str(i) for i in range(6)]\n",
    "sns.set(color_codes=True)\n",
    "cmap = sns.light_palette((260, 75, 60), input=\"husl\")\n",
    "ax = sns.heatmap(dfHn, cmap = cmap)\n",
    "l = ax.set_xticklabels(req_cols, rotation = 90)\n",
    "l = ax.set_yticklabels(igl, rotation= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3QZQi7vM0QO7"
   },
   "outputs": [],
   "source": [
    "TOP_N = 3\n",
    "iglist = []\n",
    "itemlist = []\n",
    "desclist = []\n",
    "for ig in range(OPTIMAL_EMBEDDING_SIZE):\n",
    "  dom_itms = np.argsort(H[ig])[:TOP_N]\n",
    "  for itm in dom_itms:\n",
    "    iglist.append(\"Item Group_\" + str(ig))\n",
    "    itemlist.append(itm)\n",
    "    desclist.append(lusc[req_cols[itm]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLYyRfc-1nn_"
   },
   "outputs": [],
   "source": [
    "df_dict_profile = pd.DataFrame(data = {\"Item_group\": iglist, \"Item_ID\": itemlist, \"Description\": desclist})\n",
    "df_dict_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3ToNbWogkHr"
   },
   "outputs": [],
   "source": [
    "for k,v in topic_features.items():\n",
    "    item_desc = [lusc[i] for i in v]\n",
    "    print(\"Item Group %d, elements:\" %(k+1))\n",
    "    for i, desc in enumerate(item_desc):\n",
    "        print(desc)\n",
    "        if i >=4:\n",
    "            break\n",
    "    print()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-GCl7y8gkHr"
   },
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrKO-q8k5zsD"
   },
   "source": [
    "## Customer Profile\n",
    "\n",
    "To develop the customer profile, the interpretation of the customer representation as his or her affinity towards the $k$ latent groups is utilized. The set $k$ dimensional customer representations are clustered using a density clustering algorithm $\\textbf{hdbscan}$. This algorithm can discriminate between points that genuinely cluster together versus noise. Many real world datasets present with background noise. Removing this background noise helps us identify the actual clusters in the dataset. It turns out that this dataset has a large noise cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9n2uF5ZgkHr"
   },
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN(min_samples=5)\n",
    "clusterer.fit(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiyqCxcd6mMu"
   },
   "source": [
    "### There are $6$ clusters in data apart from the noise cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4aOhW1I1gkHs"
   },
   "outputs": [],
   "source": [
    "np.unique(clusterer.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwNJesFwe7IT"
   },
   "outputs": [],
   "source": [
    "clusterer.labels_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPTzfAm-gkHs"
   },
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8egSL2AgkHs"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(n_components=2, random_state=1234)\n",
    "Xt = model.fit_transform(X) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBQ2-5wzgkHt"
   },
   "outputs": [],
   "source": [
    "dft = pd.DataFrame(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbJtIluIgkHt"
   },
   "outputs": [],
   "source": [
    "dft[\"cluster\"] = clusterer.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHbrs63U6upE"
   },
   "source": [
    "## Profile of the clusters\n",
    "* There is a large noise cluster\n",
    "* There is a large cluster (cluster label $4$) and $5$ smaller clusters. The cluster sizes are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4M8ufspxgkHt"
   },
   "outputs": [],
   "source": [
    "cluster_counts = dft[\"cluster\"].value_counts()\n",
    "cluster_counts.plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UiSk8Kj3gkHt"
   },
   "outputs": [],
   "source": [
    "dfnf = dft.query(\"cluster != -1\")\n",
    "dfnf.columns = [\"X\", \"Y\", \"cluster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5KXQL43yNi-"
   },
   "outputs": [],
   "source": [
    "dfW = pd.DataFrame(W)\n",
    "dfW.columns = [\"Item Group_\"+ str(i) for i in range(5)]\n",
    "dfW[\"cluster\"] = clusterer.labels_\n",
    "dfW = dfW.query(\"cluster != -1\")\n",
    "dfWc = dfW.groupby(by = [\"cluster\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zYpWZUk7TOJ"
   },
   "outputs": [],
   "source": [
    "colsneeded = [\"Item Group_\"+ str(i) for i in range(5)]\n",
    "dfWc = dfWc[colsneeded]\n",
    "dfWc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_BcGoK64xLE"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "Xc = preprocessing.normalize(dfWc.values)\n",
    "dfWcn = pd.DataFrame(Xc)\n",
    "dfWcn.columns = colsneeded\n",
    "dfWcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xj3thBnNjyj6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCalzY0qcwyx"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,9.5)) \n",
    "ccl = [\"Cluster-\" + str(i) for i in range(6)]\n",
    "sns.set(color_codes=True)\n",
    "cmap = sns.light_palette(\"purple\")\n",
    "ax = sns.heatmap(dfWcn, cmap = cmap)\n",
    "l = ax.set_xticklabels(colsneeded, rotation = 90)\n",
    "l = ax.set_yticklabels(ccl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uAqq8vQ_TJN"
   },
   "source": [
    "## Plot the Customer Clusters using tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mB9Pe3CJgkHt"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [11, 8]\n",
    "sns.set(color_codes=True)\n",
    "Xnf= dfnf[\"X\"].values\n",
    "Ynf = dfnf[\"Y\"].values\n",
    "sns.scatterplot(Xnf, Ynf, hue = dfnf[\"cluster\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ny3wQgHC_aSu"
   },
   "source": [
    "## Frequent Customer Graph\n",
    "The graph that captures frequent store customers and their purchasing activity are represented as a graph. This graph and the embeddings of the customers as well as items are stored in ArangoDB. The details of doing so are provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0Lo8WDsgkHt"
   },
   "outputs": [],
   "source": [
    "\n",
    "from arangopipe.arangopipe_storage.arangopipe_api import ArangoPipe\n",
    "from arangopipe.arangopipe_storage.arangopipe_admin_api import ArangoPipeAdmin\n",
    "from arangopipe.arangopipe_storage.arangopipe_config import ArangoPipeConfig\n",
    "from arangopipe.arangopipe_storage.managed_service_conn_parameters import ManagedServiceConnParam\n",
    "mdb_config = ArangoPipeConfig()\n",
    "msc = ManagedServiceConnParam()\n",
    "conn_params = { msc.DB_SERVICE_HOST : \"arangoml.arangodb.cloud\", \\\n",
    "                        msc.DB_SERVICE_END_POINT : \"createDB\",\\\n",
    "                        msc.DB_SERVICE_NAME : \"createDB\",\\\n",
    "                        msc.DB_SERVICE_PORT : 8529,\\\n",
    "                        msc.DB_CONN_PROTOCOL : 'https',\\\n",
    "                        msc.DB_REPLICATION_FACTOR: 3}\n",
    "        \n",
    "mdb_config = mdb_config.create_connection_config(conn_params)\n",
    "admin = ArangoPipeAdmin(reuse_connection = False, config = mdb_config)\n",
    "ap_config = admin.get_config()\n",
    "ap = ArangoPipe(config = ap_config)\n",
    "proj_info = {\"name\": \"Retail_Graph_Analytics\"}\n",
    "proj_reg = admin.register_project(proj_info)\n",
    "mdb_config.get_cfg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "941K7uLOr264"
   },
   "outputs": [],
   "source": [
    "\n",
    "ds_info = {\"name\" : \"Frequent_Customer_Graph\",\\\n",
    "            \"description\": \"Frequent Customer Graph of Retail Dataset\",\\\n",
    "           \"source\": \"ArangoDB dump of Retail from Notebook II\" }\n",
    "ds_reg = ap.register_dataset(ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bGoynZ7sed8"
   },
   "outputs": [],
   "source": [
    "featureset = {\"name\": \"FC_NO_FS\"}\n",
    "fs_reg = ap.register_featureset(featureset, ds_reg[\"_key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goI1AxcPsxyu"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "url = ('https://raw.githubusercontent.com/arangodb/interactive_tutorials/master/notebooks/Matrix_Factorization_Freq_Shoppers.ipynb')\n",
    "nbjson = requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tq7QJ7ZdtPRj"
   },
   "outputs": [],
   "source": [
    "model_info = {\"name\": \"MF for Frequent Customers\",  \"task\": \"Develop Embeddings\", 'notebook': nbjson}\n",
    "model_reg = ap.register_model(model_info, project = \"Retail_Graph_Analytics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JEosKvntajR"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import datetime\n",
    "#import jsonpickle\n",
    "\n",
    "ruuid = str(uuid.uuid4().int)\n",
    "model_perf = {'run_id': ruuid, \"timestamp\": str(datetime.datetime.now())}\n",
    "\n",
    "#mp = jsonpickle.encode(mp)\n",
    "model_params = {'run_id': ruuid, 'model_params': 'Not Applicable'}\n",
    "\n",
    "run_info = {\"dataset\" : ds_reg[\"_key\"],\\\n",
    "                    \"featureset\": fs_reg[\"_key\"],\\\n",
    "                    \"run_id\": ruuid,\\\n",
    "                    \"model\": model_reg[\"_key\"],\\\n",
    "                    \"model-params\": model_params,\\\n",
    "                    \"model-perf\": model_perf,\\\n",
    "                    \"tag\": \"Retail Descriptive Analytics Notebook\",\\\n",
    "                    \"project\": \"Retail_Graph_Analytics\"}\n",
    "ap.log_run(run_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDwSfvHsIS6S"
   },
   "outputs": [],
   "source": [
    "kg_db = admin.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ik13Jkc_3Xl"
   },
   "source": [
    "## Create the collections to store the frequent customer graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPPif9BFIqkn"
   },
   "outputs": [],
   "source": [
    "\n",
    "# create a new graph called freq_cust_graph in the temp database if it does not already exist.\n",
    "if not kg_db.has_graph(\"freq_cust_graph\"):\n",
    "    kg_db.create_graph('freq_cust_graph', smart=True)\n",
    "\n",
    "# # create a new graph called user_user_graph in the temp database if it does not already exist.\n",
    "# if not kg_db.has_graph(\"user_user_graph\"):\n",
    "#     kg_db.create_graph(\"user_user_graph\", smart=True)\n",
    "\n",
    "# # This returns and API wrapper for the above created graphs\n",
    "freq_cust_graph = kg_db.graph(\"freq_cust_graph\")\n",
    "# user_user_graph = kg_db.graph(\"user_user_graph\")\n",
    "\n",
    "# create a new collection named \"Customers\" if it does not exist.\n",
    "# This returns an API wrapper for \"Customers\" collection.\n",
    "if not kg_db.has_collection(\"Customers\"):\n",
    "    kg_db.create_collection(\"Customers\", replication_factor=3)\n",
    "\n",
    "# Create a new vertex collection named \"Customers\" if it does not exist.\n",
    "if not freq_cust_graph.has_vertex_collection(\"Customers\"):\n",
    "    freq_cust_graph.vertex_collection(\"Customers\")\n",
    "\n",
    "# create a new collection named \"Items\" if it does not exist.\n",
    "# This returns an API wrapper for \"Items\" collection.\n",
    "if not kg_db.has_collection(\"Items\"):\n",
    "    kg_db.create_collection(\"Items\", replication_factor=3)\n",
    "\n",
    "# Create a new vertex collection named \"Items\" if it does not exist.\n",
    "if not freq_cust_graph.has_vertex_collection(\"Items\"):\n",
    "    freq_cust_graph.vertex_collection(\"Items\")\n",
    "\n",
    "# create a new collection named \"Purchases\" if it does not exist.\n",
    "# This returns an API wrapper for \"Purchases\" collection.\n",
    "if not kg_db.has_collection(\"Purchases\"):\n",
    "    kg_db.create_collection(\"Purchases\", edge=True, replication_factor=3)\n",
    "\n",
    "# creating edge definitions named \"Purchases\". This creates any missing\n",
    "# collections and returns an API wrapper for \"Purchases\" edge collection.\n",
    "if not freq_cust_graph.has_edge_definition(\"Purchases\"):\n",
    "    Purchases = freq_cust_graph.create_edge_definition(\n",
    "        edge_collection='Purchases',\n",
    "        from_vertex_collections=['Customers'],\n",
    "        to_vertex_collections=['Items']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDFgef8T6Z3O"
   },
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEjPZKoGAAGu"
   },
   "source": [
    "### Store the customers and their embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Daz665EvCU3H"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "%time\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.2f')\n",
    "doc_index = 0\n",
    "\n",
    "batch = []\n",
    "BATCH_SIZE = 100\n",
    "batch_idx = 1\n",
    "collection =kg_db[\"Customers\"]\n",
    "cust_cluster = clusterer.labels_\n",
    "\n",
    "for count, value in enumerate(cust_ids):\n",
    "  insert_doc = {}\n",
    "  user_id = \"Customers/\" + str(value)\n",
    "  insert_doc[\"_id\"] = user_id\n",
    "  insert_doc[\"mf_emb\"] = np.asarray(W[count], dtype = np.float32).tolist()\n",
    "  insert_doc[\"RFM_score\"] = str(rfm_scores[count])\n",
    "  insert_doc[\"cluster\"] = str(cust_cluster[count])\n",
    "  batch.append(insert_doc)\n",
    "  doc_index += 1\n",
    "  last_record = (count == (df.shape[0] - 1)) \n",
    "  if doc_index % BATCH_SIZE == 0:\n",
    "      print(\"Inserting batch %d\" % (batch_idx))\n",
    "      batch_idx += 1\n",
    "      collection.import_bulk(batch)\n",
    "      batch = []\n",
    "  if last_record and len(batch) > 0:\n",
    "      print(\"Inserting batch the last batch!\")\n",
    "      collection.import_bulk(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKrSsHTRAFOL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSy42IOBAFV6"
   },
   "source": [
    "### Stoe the store inventory items and their embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOapu6CQL41p"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "%time\n",
    "\n",
    "doc_index = 0\n",
    "\n",
    "batch = []\n",
    "BATCH_SIZE = 30\n",
    "batch_idx = 1\n",
    "collection =kg_db[\"Items\"]\n",
    "Ht = H.T\n",
    "for count, value in enumerate(req_cols):\n",
    "  insert_doc = {}\n",
    "  item_id = \"Items/\" + str(value)\n",
    "  insert_doc[\"_id\"] = item_id\n",
    "  insert_doc[\"mf_emb\"] = np.asarray(Ht[count], dtype = np.float32).tolist()\n",
    "  insert_doc[\"desc\"] = lusc[value]\n",
    "  batch.append(insert_doc)\n",
    "  doc_index += 1\n",
    "  last_record = (count == (len(req_cols) - 1)) \n",
    "  if doc_index % BATCH_SIZE == 0:\n",
    "      print(\"Inserting batch %d\" % (batch_idx))\n",
    "      batch_idx += 1\n",
    "      collection.import_bulk(batch)\n",
    "      batch = []\n",
    "  if last_record and len(batch) > 0:\n",
    "      print(\"Inserting batch the last batch!\")\n",
    "      collection.import_bulk(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRHsVLD3ALTf"
   },
   "source": [
    "### Store purchasing activity (captured as an edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_17W43u59kq"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "%time\n",
    "\n",
    "doc_index = 0\n",
    "\n",
    "batch = []\n",
    "BATCH_SIZE = 500\n",
    "batch_idx = 1\n",
    "collection =kg_db[\"Purchases\"]\n",
    "# returns all the edges from t10cu nodes to the nodes which are at 1-hop distance \n",
    "\n",
    "for index, row in df.iterrows():\n",
    "  for col in req_cols:\n",
    "    if row[col] > 0:\n",
    "      user_id = \"Customers/\" + str(cust_ids[index])\n",
    "      item_id = \"Items/\" + str(col)\n",
    "      insert_doc = {\"_from\": user_id, \"_to\": item_id}\n",
    "      batch.append(insert_doc)\n",
    "      doc_index += 1\n",
    "  last_record = (index == (df.shape[0] - 1)) \n",
    "  if doc_index % BATCH_SIZE == 0:\n",
    "      print(\"Inserting batch %d\" % (batch_idx))\n",
    "      batch_idx += 1\n",
    "      collection.import_bulk(batch)\n",
    "      batch = []\n",
    "  if last_record and len(batch) > 0:\n",
    "      print(\"Inserting batch the last batch!\")\n",
    "      collection.import_bulk(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyrR2icdt3Th"
   },
   "source": [
    "## Query to find customers similar to a customer\n",
    "\n",
    "### AQL Query (can be run through the web UI queries tab)\n",
    "The following query can be used to find customers similar to a customer\n",
    "```/*locate specific user*/\n",
    "LET uemb = (\n",
    "FOR u in Customers\n",
    "    FILTER u._id == \"Customers/12748\"\n",
    "    FOR j in RANGE(0,4)\n",
    "        RETURN TO_NUMBER(NTH(u.mf_emb,j))\n",
    "    )\n",
    "\n",
    "/*calculate distance from user to all other users*/\n",
    "LET dau = (\n",
    "    FOR v in Customers\n",
    "    /* Limit to users that have an embedding*/\n",
    "    FILTER HAS(v, \"mf_emb\")\n",
    "    LET dv = (SQRT(SUM(\n",
    "    \n",
    "        FOR i in RANGE(0,4)\n",
    "            LET di = TO_NUMBER(NTH(uemb, i)) - TO_NUMBER(NTH(v.mf_emb, i))\n",
    "            RETURN POW(di,2)\n",
    "        )))\n",
    "    RETURN {\"user\": v._id, \"dist\": dv}\n",
    "    )\n",
    "/*sort results*/    \n",
    "FOR du in dau\n",
    "    SORT du.dist\n",
    "    RETURN {\"user\": du.user, \"dist\": du.dist}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hom5rs5pr5aU"
   },
   "source": [
    "![](https://github.com/arangodb/interactive_tutorials/blob/retail_data_branch/notebooks/img/mf_cust_similarity_query.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCS6POAauAIE"
   },
   "source": [
    "## Query to find items similar to an item\n",
    "\n",
    "### AQL Query (can be run through the web UI queries tab)\n",
    "The following query can be used to find items  similar to an item\n",
    "\n",
    "```\n",
    "LET uemb = (\n",
    "FOR u in Items\n",
    "    FILTER u._id == \"Items/22045\"\n",
    "    FOR j in RANGE(0,4)\n",
    "        RETURN TO_NUMBER(NTH(u.mf_emb,j))\n",
    "    )\n",
    "\n",
    "/*calculate distance from item to all other items*/\n",
    "LET dau = (\n",
    "    FOR v in Items\n",
    "    /* Limit to items that have an embedding*/\n",
    "    FILTER HAS(v, \"mf_emb\")\n",
    "    LET dv = (SQRT(SUM(\n",
    "    \n",
    "        FOR i in RANGE(0,4)\n",
    "            LET di = TO_NUMBER(NTH(uemb, i)) - TO_NUMBER(NTH(v.mf_emb, i))\n",
    "            RETURN POW(di,2)\n",
    "        )))\n",
    "    RETURN {\"item\": v._id, \"dist\": dv}\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kZToxa5sm1l"
   },
   "source": [
    "![](https://github.com/arangodb/interactive_tutorials/blob/retail_data_branch/notebooks/img/mf_item_similarity_query.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCPJHoaquEP3"
   },
   "source": [
    "## Query to find customers in a cluster\n",
    "\n",
    "### AQL Query (can be run through the web UI queries tab)\n",
    "The following query can be used to find customers in a cluster\n",
    "\n",
    "```\n",
    "for c in Customers\n",
    "    FILTER c.cluster == '4'\n",
    "        RETURN c \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYqPICJzsuH_"
   },
   "source": [
    "![](https://github.com/arangodb/interactive_tutorials/blob/retail_data_branch/notebooks/img/mf_emb_cluster4_cust.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yS33IqtsRD5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Matrix_Factorization_Freq_Shoppers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
