{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FuzzySearch.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfxiSjWR8Kmw"
      },
      "source": [
        "![arangodb](https://github.com/joerg84/ArangoDBUniversity/blob/master/img/ArangoDB_logo.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE3KR8sW8Kmw"
      },
      "source": [
        "# ArangoSearch - 3.8 Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3DiEFJE8Kmx"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arangodb/interactive_tutorials/blob/master/notebooks/Search_3_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkdJgoJ4Nu_5"
      },
      "source": [
        "Besides many other exciting [features, ArangoDB 3.8](https://www.arangodb.com/docs/3.8/release-notes-new-features38.html)  added a number of new features for ArangoSearch which you can explore in this notebook:\n",
        "* Pipeline Analyzer\n",
        "* AQL Analyzer\n",
        "* Geo-spatial queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o7Vn4Uo8Kmy"
      },
      "source": [
        "Just in case you are new to ArangoDB (or ArangoSearch): [ArangoSearch](https://www.arangodb.com/why-arangodb/full-text-search-engine-arangosearch/) provides information retrieval features, natively integrated into ArangoDB’s query language and with support for all data models. It is primarily a full-text search engine, a much more powerful alternative to the full-text index type.\n",
        "Check this [ArangoSearch notebook](https://colab.research.google.com/github/joerg84/ArangoDBUniversity/blob/master/ArangoSearch.ipynb) for an introduction to ArangoSearch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlcbVfOs8Kmy"
      },
      "source": [
        "# Setup "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoIFBPwp8Kmy"
      },
      "source": [
        "Before getting started with ArangoSearch we need to prepare our environment and create a temporary database on ArangoDB's managed Service Oasis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXdL1FZe8Kmz"
      },
      "source": [
        "%%capture\n",
        "!git clone -b oasis_connector --single-branch https://github.com/arangodb/interactive_tutorials.git\n",
        "!rsync -av interactive_tutorials/ ./ --exclude=.git\n",
        "!chmod -R 755 ./tools\n",
        "!git clone -b arangobnb_no_calendar --single-branch https://github.com/arangodb/interactive_tutorials.git arangobnb_no_calendar\n",
        "!rsync -av arangobnb_no_calendar/data/arangobnb_no_calendar/ ./arangobnb/\n",
        "!pip3 install pyarango\n",
        "!pip3 install \"python-arango>=5.0\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pKXjdTS8Km2",
        "outputId": "6857e42a-b42d-417b-e8ca-1063e05f0215"
      },
      "source": [
        "import json\n",
        "import requests\n",
        "import sys\n",
        "import oasis\n",
        "import time\n",
        "import textwrap\n",
        "\n",
        "from pyArango.connection import *\n",
        "from arango import ArangoClient"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6968hvSs8Km3"
      },
      "source": [
        "Create the temporary database:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaGHLin28Km4",
        "outputId": "85921c8c-eaf4-4acc-fecd-dd746850a921"
      },
      "source": [
        "# Retrieve tmp credentials from ArangoDB Tutorial Service\n",
        "login = oasis.getTempCredentials(tutorialName=\"ArangoSearch_3_8\", credentialProvider=\"https://tutorials.arangodb.cloud:8529/_db/_system/tutorialDB/tutorialDB\")\n",
        "\n",
        "# Connect to the temp database\n",
        "# Please note that we use the python-arango driver as it has better support for ArangoSearch \n",
        "database = oasis.connect_python_arango(login)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requesting new temp credentials.\n",
            "Temp database ready to use.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECAfpWU48Km6",
        "outputId": "a03e6eb7-4312-4de7-b97f-8c364278dc6d"
      },
      "source": [
        "print(\"https://\"+login[\"hostname\"]+\":\"+str(login[\"port\"]))\n",
        "print(\"Username: \" + login[\"username\"])\n",
        "print(\"Password: \" + login[\"password\"])\n",
        "print(\"Database: \" + login[\"dbName\"])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://tutorials.arangodb.cloud:8529\n",
            "Username: TUT5w4y8hfglq3me5ict4kn1h\n",
            "Password: TUTw6pql9oo8id97ldravge4\n",
            "Database: TUTs2hif4g0bnfbs2ytwf77ti\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ0mydlq8Km8"
      },
      "source": [
        "Feel free to use the above URL to checkout the ArangoDB WebUI!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vXqUK6L8Km9"
      },
      "source": [
        "#  IMDB Example Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9JQShDi8Km9"
      },
      "source": [
        "![imdb](https://github.com/joerg84/ArangoDBUniversity/blob/master/img/IMDB_graph.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXkaey-g8Km9"
      },
      "source": [
        "Last, but not least we will import the [IMDB Example Dataset](https://github.com/arangodb/example-datasets/tree/master/Graphs/IMDB) including information about various movies, actors, directors, ... as a graph. \n",
        "*Note the included arangorestore will only work on Linux or Windows systems, if you want to run this notebook on a different OS please consider using the appropriate arangorestore from the [Download area](https://www.arangodb.com/download-major/) and for more information on how to use the ArangoDB client tools, [see the documentation](https://www.arangodb.com/docs/stable/programs.htmlhttps://www.arangodb.com/docs/stable/programs.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKM6jcXa8Km-"
      },
      "source": [
        "## Linux:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPqigG8H8Km-",
        "outputId": "307083f0-7777-40c3-ded8-8c543ef5d4c9"
      },
      "source": [
        "! ./tools/arangorestore -c none --server.endpoint http+ssl://{login[\"hostname\"]}:{login[\"port\"]} --server.username {login[\"username\"]} --server.database {login[\"dbName\"]} --server.password {login[\"password\"]} --default-replication-factor 3  --input-directory \"arangobnb\""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m2021-06-21T20:37:05Z [231] INFO [05c30] {restore} Connected to ArangoDB 'http+ssl://tutorials.arangodb.cloud:8529'\n",
            "\u001b[0m\u001b[0m2021-06-21T20:37:07Z [231] INFO [abeb4] {restore} Database name in source dump is 'arangobnb'\n",
            "\u001b[0m\u001b[0m2021-06-21T20:37:07Z [231] INFO [9b414] {restore} # Re-creating document collection 'listings'...\n",
            "\u001b[0m\u001b[31m2021-06-21T20:37:07Z [231] ERROR [e8e7a] {restore} Error while creating document collection 'listings': got invalid response from server: HTTP 400: 'replicationFactor must not be lower than minimum allowed replicationFactor (3)' while executing 'restoring collection' with this payload: '{\"indexes\":[{\"id\":\"7881013\",\"type\":\"persistent\",\"name\":\"hostID\",\"fields\":[\"host_id\"],\"unique\":false,\"sparse\":false,\"deduplicate\":false,\"estimates\":true},{\"id\":\"8283819\",\"type\":\"geo\",\"name\":\"location\",\"fields\":[\"location\"],\"unique\":false,\"sparse\":true,\"maxNumCoverCells\":8,\"worstIndexedLevel\":4,\"bestIndexedLevel\":17,\"geoJson\":true}],\"parameters\":{\"allowUserKeys\":true,\"cacheEnabled\":false,\"cid\":\"357\",\"deleted\":false,\"globallyUniqueId\":\"hBBD5E7538BF1/7874214\",\"id\":\"357\",\"isDisjoint\":false,\"isSmart\":false,\"isSmartChild\":false,\"isSystem\":false,\"keyOptions\":{\"allowUserKeys\":true,\"type\":\"traditional\",\"lastValue\":47156730},\"minReplicationFactor\":1,\"minRevision\":\"_b06jvpK---\",\"name\":\"listings\",\"numberOfShards\":1,\"planId\":\"7874214\",\"replicationFactor\":1,\"schema\":null,\"shardKeys\":[\"_key\"],\"shards\":{},\"status\":3,\"syncByRevision\":true,\"type\":2,\"usesRevisionsAsDocumentIds\":true,\"version\":9,\"waitForSync\":false,\"writeConcern\":1}}'\n",
            "\u001b[0m\u001b[31m2021-06-21T20:37:07Z [231] ERROR [cb69f] {restore} got invalid response from server: HTTP 400: 'replicationFactor must not be lower than minimum allowed replicationFactor (3)' while executing 'restoring collection' with this payload: '{\"indexes\":[{\"id\":\"7881013\",\"type\":\"persistent\",\"name\":\"hostID\",\"fields\":[\"host_id\"],\"unique\":false,\"sparse\":false,\"deduplicate\":false,\"estimates\":true},{\"id\":\"8283819\",\"type\":\"geo\",\"name\":\"location\",\"fields\":[\"location\"],\"unique\":false,\"sparse\":true,\"maxNumCoverCells\":8,\"worstIndexedLevel\":4,\"bestIndexedLevel\":17,\"geoJson\":true}],\"parameters\":{\"allowUserKeys\":true,\"cacheEnabled\":false,\"cid\":\"357\",\"deleted\":false,\"globallyUniqueId\":\"hBBD5E7538BF1/7874214\",\"id\":\"357\",\"isDisjoint\":false,\"isSmart\":false,\"isSmartChild\":false,\"isSystem\":false,\"keyOptions\":{\"allowUserKeys\":true,\"type\":\"traditional\",\"lastValue\":47156730},\"minReplicationFactor\":1,\"minRevision\":\"_b06jvpK---\",\"name\":\"listings\",\"numberOfShards\":1,\"planId\":\"7874214\",\"replicationFactor\":1,\"schema\":null,\"shardKeys\":[\"_key\"],\"shards\":{},\"status\":3,\"syncByRevision\":true,\"type\":2,\"usesRevisionsAsDocumentIds\":true,\"version\":9,\"waitForSync\":false,\"writeConcern\":1}}'\n",
            "\u001b[0m\u001b[0m2021-06-21T20:37:07Z [231] INFO [a66e1] {restore} Processed 0 collection(s) in 2.210109 s, read 0 byte(s) from datafiles, sent 0 data batch(es) of 0 byte(s) total size\n",
            "\u001b[0m"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqPRu3HiTw2L"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yE_uMym8KnA"
      },
      "source": [
        "# Create First View"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhQynyJJ8KnB"
      },
      "source": [
        "As discussed above, an ArangoSearch view contains references to documents stored in different collections. \n",
        "This makes it possible to perform complex federated searches, even over a complete graph including vertex and edge collections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-koXo6C8KnB"
      },
      "source": [
        "# Create an ArangoSearch view.\n",
        "database.create_arangosearch_view(\n",
        "    name='v_imdb'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn3rKYKG8KnD"
      },
      "source": [
        "Let us check it is actually there:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5bwOthX8KnD"
      },
      "source": [
        "print(database[\"v_imdb\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7RJtPcu8KnF"
      },
      "source": [
        "Next, we will create a [custom analyzer](https://www.arangodb.com/docs/stable/arangosearch-analyzers.html) to preprocess the values.\n",
        "Note that, in order to support n-gram similarity the analyzer must have:\n",
        "* At least the \"position\" and \"frequency\" features enabled\n",
        "* The same min and max values\n",
        "* preserveOriginal set to False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSfISdj8kpd6"
      },
      "source": [
        "# Delete in case analyzer existed before\n",
        "database.delete_analyzer('fuzzy_search_bigram', ignore_missing=True)\n",
        "\n",
        "database.create_analyzer(\n",
        "        name='fuzzy_search_bigram',\n",
        "        analyzer_type='ngram',\n",
        "        properties={  \n",
        "        \"min\": 2,  \n",
        "        \"max\": 2,  \n",
        "        \"preserveOriginal\": False \n",
        "        }, \n",
        "        features=[\"position\", \"frequency\", \"norm\"] \n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiCHaBtl4gkZ"
      },
      "source": [
        "Next, we need to link the view and our custom analyzer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL8GQTtQ8KnF"
      },
      "source": [
        " link = { \n",
        "  \"includeAllFields\": True,\n",
        "  \"fields\" : { \n",
        "      \"title\" : { \"analyzers\" : [ \"fuzzy_search_bigram\"] },\n",
        "      \"description\" : { \"analyzers\" : [ \"fuzzy_search_bigram\"] }\n",
        "      }\n",
        "}\n",
        "\n",
        "\n",
        "database.update_arangosearch_view(\n",
        "    name='v_imdb',\n",
        "    properties={'links': { 'imdb_vertices': link }}\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMZumewz8KnH"
      },
      "source": [
        "As the indexing might take a few seconds, let us have a brief look at what is actually going on.\n",
        "\n",
        "When you link a collection to an ArangoSearch View, you can choose which individual fields to link or specify to link all fields. It might be helpful to think about linking fields in the same way you think about indexing attributes, although not exactly the same. When you link data to a view it is indexed in a way that allows for quick retrieval. This process also stores the data in a way that allows for the ArangoSearch-specific AQL functions to perform unique queries such as tokenizing, stemming, removing stop words, and as we will see in this notebook complex matching functions.\n",
        "\n",
        "An additional benefit and a difference to typical indexing is that you are able to link multiple collections to one view and apply the desired analyzers. The image below shows how the collections are linked, analyzed and then made available via the view. When performing queries you can use all the typical AQL functions against a view, the same way that you would with a collection name. Though, the real benefit comes when using ArangoSearch-specific functions and you start taking advantage of features such as ranking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89Xus7i28KnI"
      },
      "source": [
        "![ArangoSearch](https://github.com/joerg84/ArangoDBUniversity/blob/master/img/ArangoSearch_Arch.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8LsVW-LNIIU"
      },
      "source": [
        "By now our view should be ready, so let us issue the first test query and look for short Comedy Movies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53oLsRFJNIZV"
      },
      "source": [
        "cursor = database.aql.execute(\n",
        "  \"\"\"\n",
        "  FOR d IN v_imdb \n",
        "    SEARCH d.type == \"Movie\" \n",
        "    AND \n",
        "    d.genre == \"Comedy\" \n",
        "    AND \n",
        "    d.runtime IN 10..50 \n",
        "    RETURN d.title\n",
        "  \"\"\"\n",
        ")\n",
        "# Iterate through the result cursor\n",
        "print('\\033[4mMovie Titles\\033[0m ')\n",
        "\n",
        "for doc in cursor:\n",
        "  print(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFhVZYhcSejU"
      },
      "source": [
        "If we set up everything correctly there should be 13 results, containing comedies that are less than 50 minutes long, such as:\n",
        " * Finders Keepers\n",
        " * Stuart Dee\n",
        " * The Pawnshop\n",
        " * Robot Wrecks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kInsH7LYSiEF"
      },
      "source": [
        "Now that we have finished some of the setup, let's move on to the functions that make up Fuzzy search. \n",
        "As mentioned in the beginning of this notebook, Fuzzy search comes in the form of various [N-Gram Similarity](https://www.arangodb.com/docs/stable/aql/functions-arangosearch.html#ngram_match) and [Levenshtein distance](https://www.arangodb.com/docs/stable/aql/functions-arangosearch.html#levenshtein_match\n",
        ") AQL functions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF3oT3gQfbB0"
      },
      "source": [
        "# N-Gram Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haexui4sDce2"
      },
      "source": [
        "`NGRAM_SIMILARITY(input, target, ngramSize) → similarity`\n",
        "\n",
        "N-gram similarity is a measure for the difference between two strings represented by counting how long the longest sequence of matching n-grams is, divided by target’s total n-gram count. To better understand this concept let's start with a simple example. The below query compares the phrase `quick fox` to the similar phrase of `quick foxx` (additional `x`). These are similar phrases and as such, they should have a high n-gram similarity. \n",
        "\n",
        "```\n",
        "Case Conversion Note:\n",
        "N-gram analyzers do not currently support case conversion. \n",
        "It's definitely worthwhile to have the input match the stored terms case, but even without it NGRAM_MATCH can work, but will be less accurate\n",
        "```\n",
        "\n",
        "Go ahead and execute the query below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4-VhEhgC6oM"
      },
      "source": [
        "cursor = database.aql.execute(\n",
        "\"\"\"\n",
        "RETURN NGRAM_SIMILARITY(\n",
        "\"quick fox\",\n",
        "\"quick foxx\", \n",
        "2)\"\"\"\n",
        ")\n",
        "# Iterate through the result cursor\n",
        "for doc in cursor:\n",
        "  print('\\033[4mNGRAM_SIMILARITY\\033[0m ')\n",
        "  print(doc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQlOtFJCfglL"
      },
      "source": [
        "With an n-gram size of 2, the n-gram similarity between both strings is 0.888, the closer the similarity is to 1 the more similar they are. Feel free experiment with other combinations such as `NGRAM_SIMILARITY( \"same string\",\"same string\", 2)` or vary the ngramSize.\n",
        "\n",
        "N-gram functions such as this break apart the words using the supplied ngram size, 2 in our query above. This means that the function compares the two words broken up into their 2 letter n-grams:\n",
        "\n",
        "  ```\n",
        "  quick fox         --         quick foxx\n",
        "  ----------------------------------------\n",
        "  qu                --         qu (match)\n",
        "  ui                --         ui (match)\n",
        "  ic                --         ic (match)\n",
        "  ck                --         ck (match)\n",
        "  k                 --         k  (match)\n",
        "   f                --          f (match)\n",
        "  fo                --         fo (match)\n",
        "  ox                --         ox (match)\n",
        "  x                 --         xx (do not match)\n",
        "  ```\n",
        "If we use simple math here we can see that there is around an 85% match when an extra `x` is supplied. However, n-gram similarity and distance is not as simple as this, but hopefully this provides a quick intro to the basic concept of ngram matching and similarity. If you would like to take a deep dive into this topic, a paper published by [Grzegorz Kondrak at the University of Alberta](https://webdocs.cs.ualberta.ca/~kondrak/papers/spire05.pdf) is a great resource."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POtg36jBTh5r"
      },
      "source": [
        "### N-Gram Positional Similarity\n",
        "`NGRAM_POSITIONAL_SIMILARITY(input, target, ngramSize) → similarity`\n",
        "\n",
        "While [NGRAM_SIMILARITY()](https://www.arangodb.com/docs/stable/aql/functions-string.html#ngram_similarity) only counts fully matching n-grams, [NGRAM_POSITIONAL_SIMILARITY()](https://www.arangodb.com/docs/stable/aql/functions-string.html#ngram_positional_similarity) also considers partially matching ones. Let us look at how that effects the returned scores. \n",
        "In this first example we are comparing `NGRAM_SIMILARITY` and `NGRAM_POSITIONAL_SIMILARITY` scores using the same two phrases as with our previous  example. These phrases are so similar that counting partial matches doesn't make any difference, thus we get the same scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1Uj0IBbTlmR"
      },
      "source": [
        "cursor = database.aql.execute(\n",
        "\"\"\"\n",
        "RETURN\n",
        "{\"NGRAM_SIMILARITY\" : NGRAM_SIMILARITY(\n",
        "\"quick fox\",\n",
        "\"quick foxx\", \n",
        "3),\n",
        "\"NGRAM_POSITIONAL_SIMILARITY\" : NGRAM_POSITIONAL_SIMILARITY(\n",
        "\"quick fox\",\n",
        "\"quick foxx\", \n",
        "3)}\"\"\"\n",
        ")\n",
        "# Iterate through the result cursor\n",
        "for doc in cursor:\n",
        "  print('\\033[4mNGRAM_SIMILARITY\\033[0m ', '\\033[4mNGRAM_POSITIONAL_SIMILARITY\\033[0m '.rjust(44))\n",
        "  print(doc['NGRAM_SIMILARITY'], str(doc['NGRAM_POSITIONAL_SIMILARITY']).rjust(25))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izAPzIB8Tpw4"
      },
      "source": [
        "If we start to change a few more letters in the phrases, the differences between the two functions becomes more clear. The score for `NGRAM_POSITIONAL_SIMILARITY` is nearly double that of `NGRAM_SIMILARITY`, due to the fact that it counted the partial matches. This provides us with some additional 'fuzziness' by allowing the matching requirement to be a bit more lenient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1CgLWTeTtSF"
      },
      "source": [
        "cursor = database.aql.execute(\n",
        "\"\"\"\n",
        "RETURN\n",
        "{\"NGRAM_SIMILARITY\" : NGRAM_SIMILARITY(\n",
        "\"quick fox\",\n",
        "\"quirky foxx\", \n",
        "3),\n",
        "\"NGRAM_POSITIONAL_SIMILARITY\" : NGRAM_POSITIONAL_SIMILARITY(\n",
        "\"quick fox\",\n",
        "\"quirky foxx\", \n",
        "3)}\"\"\"\n",
        ")\n",
        "# Iterate through the result cursor\n",
        "for doc in cursor:\n",
        "  print('\\033[4mNGRAM_SIMILARITY\\033[0m ', '\\033[4mNGRAM_POSITIONAL_SIMILARITY\\033[0m '.rjust(44))\n",
        "  print(doc['NGRAM_SIMILARITY'], str(doc['NGRAM_POSITIONAL_SIMILARITY']).rjust(25))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9KCiOw0TwSE"
      },
      "source": [
        "Depending on your requirements, the decision to count partially matching n-grams adds some 'fuzziness' that may help provide some context to your searches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bZtMbZyT4Qn"
      },
      "source": [
        "[NGRAM_SIMILARITY](https://www.arangodb.com/docs/stable/aql/functions-string.html#ngram_similarity) and [NGRAM_POSITIONAL_SIMILARITY](https://www.arangodb.com/docs/stable/aql/functions-string.html#ngram_positional_similarity) are two new functions that come with ArangoDB 3.7 and can be used to improve text searches but have a drawback of not being able to utilize the indexing benefits of views. They are still very powerful string functions and can offer a lot of functionality for text queries.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXECHlaJ3jvZ"
      },
      "source": [
        "## N-Gram Match\n",
        "\n",
        "`NGRAM_MATCH(path, target, threshold, analyzer) -> bool`\n",
        "\n",
        "However, [NGRAM_MATCH](https://www.arangodb.com/docs/stable/aql/functions-arangosearch.html#ngram_match) is able to use the indexing of ArangoSearch views and is what we will look at next.\n",
        "\n",
        "Let us start by using the [NGRAM_MATCH](https://www.arangodb.com/docs/stable/aql/functions-arangosearch.html#ngram_match) function to find a movie using a phrase supplied by the user. \n",
        "\n",
        "The main takeaway from this example is that this exact phrase does not exist in the description and the search terms have quite a few typos as well. Instead, thanks to the ngram analyzer and the NGRAM_MATCH function, the search is able to find the movie based on the relevance of the words supplied. This is a good start and at the end of the notebook we will see how combining this with Levenshtein can really help to round out your fuzzy searches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9_f99xRfvcZ"
      },
      "source": [
        "cursor = database.aql.execute(\n",
        "\"\"\"\n",
        "FOR d IN v_imdb \n",
        "  SEARCH NGRAM_MATCH(\n",
        "    d.description, \n",
        "    'rodo Same goo to Moardoor', \n",
        "    0.6, \n",
        "    'fuzzy_search_bigram'\n",
        "    )\n",
        "  LET score = BM25(d)\n",
        "  SORT score DESC\n",
        "  RETURN { \n",
        "    Title:d.title, \n",
        "    Description:d.description, \n",
        "    Score:score \n",
        "    }\n",
        "\"\"\"\n",
        ")\n",
        "# Iterate through the result cursor\n",
        "for doc in cursor:\n",
        "  print('\\033[4mTitle: ' + doc['Title'] + '\\033[0m')\n",
        "  print('\\033[4mDescription:\\033[0m ',textwrap.fill(doc['Description'], 90))\n",
        "  print('\\033[4mScore:\\033[0m ',str(doc['Score']))\n",
        "  print(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XsTF8Mtfwaj"
      },
      "source": [
        "The `NGRAM_MATCH` syntax follows typical ArangoSearch function structure. You first supply the field you would like to search, the search term(s), and then the next value is the threshold amount which must be between `0.0` and `1.0`, last is the analyzer to use on the search terms. The `.6` threshold amount is the new addition and is how much ‘fuzziness’ we are still considering to be a match.\n",
        "\n",
        "The similarity is calculated by counting how long the longest sequence of matching n-grams is, divided by the target’s total ngram count. Only fully matching n-grams are counted.\n",
        "\n",
        "The analyzer we used was configured with a min and max of 2, which means it looks at words 2 letters at a time. This is useful for determining the longest common sequence and context. The idea behind n-gram matching is searching for similar words, but not necessarily exact matches. One of the simplest ways of calculating similarity between two words is calculating the longest common sequence (LCS) of letters. The longer the LCS is the more similar the words are. However, this approach has one big disadvantage – absence of context. For example, words `connection` and `fonetica` have a long LCS (o-n-e-t-i) but very different meanings. To add some context, ngram sequences are used.\n",
        "\n",
        "Each word is split into a series of letter groups and these groups are then matched. If we use the same words, but calculate similarity based on 3-grams, an ngram with max and min of 3, we will get a better similarity measure: con-onn-nne-nec-ect-cti-tio-ion vs. fon-one-net-eti-tic-ica gives shorter LCS ( zero matches). To get rid of length differences we normalize the LCS length by word length. We calculate these matches to get a rating with a value between 0 (no match at all) and 1(fully matched). \n",
        "\n",
        "Increasing the ngram size is not always the best choice due to it also increasing the accuracy requirement of the search. Scores would be much lower for the above Star Wars search if we had chosen an ngram size of 3. We would need to decrease our threshold requirement which can have the impact of returning less relevant results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUgcAWSpdl8l"
      },
      "source": [
        "# Levenshtein"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JisqzP5dflN"
      },
      "source": [
        "ArangoDB comes with two forms of the Levenshtein matching algorithm, [LEVENSHTEIN_DISTANCE](https://www.arangodb.com/docs/stable/aql/functions-string.html#levenshtein_distance) and [LEVENSHTEIN_MATCH](https://www.arangodb.com/docs/stable/aql/functions-arangosearch.html#levenshtein_match). These AQL functions provide two similar approaches for adding 'fuzziness' to your AQL queries. While the AQL functions are similar there are some important differences, which will discuss in this section, as well as showcase some examples. \n",
        "\n",
        "### Levenshtein Distance\n",
        "`LEVENSHTEIN_DISTANCE(value1, value2) → levenshteinDistance`\n",
        "\n",
        "[Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) is a another measure for the difference between two strings represented by the  minimum number of single-character transformations required to move from one string to the other. Let us consider a concrete example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr-gqDa8de1I"
      },
      "source": [
        "cursor = database.aql.execute(\n",
        "\"\"\"\n",
        "RETURN LEVENSHTEIN_DISTANCE(\n",
        "\"The quick brown fox jumps over the lazy dog\", \n",
        "\"The quick black dog jumps over the brown fox\")\"\"\"\n",
        ")\n",
        "# Iterate through the result cursor\n",
        "for doc in cursor:\n",
        "  print(\"Edit Distance Transformations Required: \", doc)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r1uwBXLBaMk"
      },
      "source": [
        "Here we need a minimum of 13 transformations to move from one string to the other. \n",
        "Feel free to find a minimum sequence for this transformation or experiment with other combinations such as `LEVENSHTEIN_DISTANCE(\"ab\", \"ba\")`. Once the distance has been calculated it can be used in other parts of your application logic and even with the same query. \n",
        "\n",
        "This functionality provides some added control over your text analysis by handling what to do with the distance measure once you have it. However, in most situations you may prefer to find the relevance, distance, and determine if the keywords or phrases match some user supplied input, with one function or statement. This functionality is where Levenshtein Match comes in and is what we will review next.\n",
        "\n",
        "Before we go and as a nice transition to looking at Levenshtein Match, here are some key differences:\n",
        "* Distance is considered a string function, not tied to ArangoSearch\n",
        "* Distance does not take advantage of ArangoSearch indexing\n",
        "* Distance uses Damerau and treats transpositions of 2 adjacent characters atomically\n",
        "* Match does not use Damerau by default, but can be optionally enabled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm48Mt5jplnb"
      },
      "source": [
        "### Levenshtein Match\n",
        "\n",
        "\n",
        "`LEVENSHTEIN_MATCH(path, target, distance, transpositions, maxTerms) -> bool`\n",
        "\n",
        "Levenshtein match, matches documents with a Levenshtein distance lower than or equal to a distance between a document value and provided search value. This takes the power of the above Levenshtein distance function and combines it with filtering and relevance matching.\n",
        "\n",
        "```\n",
        "Analyzer Note:\n",
        "For our LEVENSHTEIN_MATCH examples we will use a text analyzer, instead of ngram, that has stemming disabled.\n",
        "Stemming is disabled as a convenience to avoid terms not matching due to a stemmed word, ie: galaxy is galaxi when stemmed.\n",
        "```\n",
        "The following code block:\n",
        "* Creates the analyzer named `en_tokenizer`\n",
        "* Updates our link definition object\n",
        "* Updates the `v_imdb` view definition\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMiFXQMToVQV"
      },
      "source": [
        " # Delete in case analyzer existed before\n",
        "database.delete_analyzer('en_tokenizer', ignore_missing=True)\n",
        "\n",
        "# Create a new english text analyzer to tokenize our text\n",
        "database.create_analyzer(\n",
        "        name='en_tokenizer',\n",
        "        analyzer_type='text',\n",
        "        properties={\n",
        "            'locale': 'en',\n",
        "            'stemming': False\n",
        "        }, \n",
        "        features=[\"position\",\"norm\", \"frequency\"] \n",
        "    )\n",
        "\n",
        "# Update the link definition object\n",
        " link = { \n",
        "  \"includeAllFields\": True,\n",
        "  \"fields\" : { \n",
        "      \"title\" : { \"analyzers\" : [ \"fuzzy_search_bigram\", \"en_tokenizer\"] },\n",
        "      \"description\" : { \"analyzers\" : [ \"fuzzy_search_bigram\", \"en_tokenizer\"] }\n",
        "      }\n",
        "}\n",
        "\n",
        "# Update the ArangoSearch view with the new link definition\n",
        "database.update_arangosearch_view(\n",
        "    name='v_imdb',\n",
        "    properties={'links': { 'imdb_vertices': link }}\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaqVAPE-ptcs"
      },
      "source": [
        "\n",
        "To continue exploring how Levenshtein Match can leverage edit distance scoring with term matching functionality, run the query below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYzxDnGn8KnM"
      },
      "source": [
        "# Execute the query\n",
        "cursor = database.aql.execute(\n",
        "  \"\"\"\n",
        "  FOR d IN v_imdb\n",
        "    SEARCH ANALYZER(LEVENSHTEIN_MATCH(\n",
        "      d.title, \n",
        "      'galxy', \n",
        "      2,\n",
        "      true,\n",
        "      3\n",
        "      ), \n",
        "    \"en_tokenizer\")\n",
        "    SORT BM25(d) DESC \n",
        "    LIMIT 10\n",
        "    RETURN {\n",
        "      \"Title\": d.title,\n",
        "      \"Score\": BM25(d)\n",
        "      }\n",
        "      \"\"\")\n",
        "# Iterate through the result cursor\n",
        "for doc in cursor:\n",
        "  print('Title: ', doc['Title'], )\n",
        "  print('Score: ', str(doc['Score']))\n",
        "  print(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnH-F3R6K-y4"
      },
      "source": [
        "The above query is an example of a user searching for a movie title but their search term contains a typo. The user intended to type `galaxy` but accidentally left out an `a`, easy mistake. Thanks to the [LEVENSHTEIN_MATCH](https://www.arangodb.com/docs/stable/aql/functions-arangosearch.html#levenshtein_match) function we have accommodated this very common scenario. \n",
        "\n",
        "The edit distance to add in an `a` would be less than `2`, which is the distance supplied in this query, so the term `galaxy` is also taken into account, not just the misspelled word.\n",
        "\n",
        "The `3` supplied here is optional and specifies the max number of terms, such as `galaxy`, to take into account. The higher this number is, the more results you are likely to get, this makes sorting by relevance very important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWo6D1topWRS"
      },
      "source": [
        "### Levenshtein Match + Phrase Search\n",
        "In practice it will be common to combine [LEVENSHTEIN_MATCH](https://www.arangodb.com/docs/stable/aql/functions-arangosearch.html#levenshtein_match) with other ArangoSearch AQL functions, [PHRASE](https://www.arangodb.com/docs/stable/aql/functions-arangosearch.html#phrase) being a likely choice.\n",
        "The Phrase function honors the position of the search term, where Levenshtein Match just looks for the word to exist and evaluates it based on relevance to the term.\n",
        "\n",
        "This combination is so common that Levenshtein Match comes with a second syntax style that works perfectly with PHRASE. The array syntax variant shown in the query below allows for omitting the initial path argument, as it is already supplied by the PHRASE function. This combination gives us the best of both worlds, precise control with the flexibility of fuzzy search.\n",
        "\n",
        "This query looks for movie titles starting with the word `star` and uses [LEVENSHTEIN_MATCH](https://www.arangodb.com/docs/stable/aql/functions-arangosearch.html#levenshtein_match) to match a second word, with Damerau transposition set to true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcgHY9pLLDHn"
      },
      "source": [
        "cursor = database.aql.execute(\n",
        "    \"\"\"\n",
        "FOR d IN v_imdb\n",
        "  SEARCH PHRASE(d.title, [ 'star', { LEVENSHTEIN_MATCH : ['wr', 2, true] } ], \"en_tokenizer\")\n",
        "    SORT BM25(d) DESC \n",
        "    RETURN d.title\n",
        "    \"\"\")\n",
        "for doc in cursor:\n",
        "  print(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbxujkbYupoM"
      },
      "source": [
        "You can continue using multiple [LEVENSHTEIN_MATCH](https://www.arangodb.com/docs/stable/aql/functions-arangosearch.html#levenshtein_match) functions even in a single Phrase statement. This makes it possible to search for phrases where every word possibly has a typo, along with the additional [PHRASE options](https://www.arangodb.com/docs/stable/aql/functions-arangosearch.html#phrase) such as skipTokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AXztvTMLGNz"
      },
      "source": [
        "cursor = database.aql.execute(\n",
        "\"\"\"\n",
        "LET phraseStructure = [ \n",
        "  { LEVENSHTEIN_MATCH : ['lrd', 1, true] }, \n",
        "  2, // offset between adjacent phrase parts\n",
        "  { LEVENSHTEIN_MATCH : ['rng', 2, true] }\n",
        "]\n",
        "FOR d IN v_imdb\n",
        "  SEARCH PHRASE(d.title, phraseStructure, \"en_tokenizer\")\n",
        "    SORT BM25(d) DESC \n",
        "    RETURN d.title\n",
        "\"\"\")\n",
        "for doc in cursor:\n",
        "  print(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH3sEI2QB21G"
      },
      "source": [
        "### Combined Fuzziness\n",
        "\n",
        "#### Differences\n",
        "\n",
        "**Levenshtein**\n",
        "\n",
        "If your searches are typically single word searches, Levenshtein match is usually the better option. With these types of searches [NGRAM_MATCH](https://www.arangodb.com/docs/stable/aql/functions-arangosearch.html#ngram_match) is not as performant and adds more overhead to CPU. Also, when indexing, ngram analyzers store positional information, which results in larger sized indexes even for single and small words.\n",
        "\n",
        "**N-Gram**\n",
        "\n",
        "N-gram is better for longer search terms. Some use cases where using n-gram functions are ideal include:\n",
        " * Genome Sequencing\n",
        " * Languages with longer words, such as German\n",
        " * Log and Sensor data that may have long connected strings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcmQcLkvy-_N"
      },
      "source": [
        "**Combination**\n",
        "\n",
        "While there are differences between the two, they can combine to bring a high level of fuzziness and accuracy to your searches.\n",
        "\n",
        "The below examples shows how you can use [NGRAM_MATCH](https://www.arangodb.com/docs/stable/aql/functions-arangosearch.html#ngram_match) to try and match portions of the words and use then [LEVENSHTEIN_MATCH](https://www.arangodb.com/docs/stable/aql/functions-arangosearch.html#levenshtein_match) to match to boost whole words and phrases. You gain the benefit of error checking and context from [NGRAM_MATCH](https://www.arangodb.com/docs/stable/aql/functions-arangosearch.html#ngram_match) while maintaining accuracy and relevance with [LEVENSHTEIN_MATCH](https://www.arangodb.com/docs/stable/aql/functions-arangosearch.html#levenshtein_match). Both functions contribute to the overall score, giving you a balance of both. \n",
        "\n",
        "Notice as well the sharp drop in the score for the movies whose description doesn't contain `Luke Skywalker`, this is BOOST and [LEVENSHTEIN_MATCH](https://www.arangodb.com/docs/stable/aql/functions-arangosearch.html#levenshtein_match) in action!\n",
        "\n",
        "Note: We parsed the `phraseStructure` separately here, just for convenience and readability. This all could have been done in-line, in the same statement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hagvz6j_HRxx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6xBEzxxHz-P"
      },
      "source": [
        "cursor = database.aql.execute(\"\"\"\n",
        "LET input = \"Luk Skywlker\"\n",
        "LET phraseStructure = [\n",
        "    {\n",
        "      \"LEVENSHTEIN_MATCH\": [\n",
        "        \"luk\",\n",
        "        3,\n",
        "        true\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"LEVENSHTEIN_MATCH\": [\n",
        "        \"skywlker\",\n",
        "        3,\n",
        "        true\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "FOR d IN v_imdb \n",
        "  SEARCH NGRAM_MATCH(d.description, input, 0.6, 'fuzzy_search_bigram') // matches part of the words to provide context\n",
        "         OR\n",
        "         BOOST(PHRASE(d.description, phraseStructure, 'en_tokenizer'), 10) // matches whole words to boost documents containing the matched words\n",
        "  SORT BM25(d) DESC  \n",
        "  RETURN {\n",
        "    \"Title\" : d.title,\n",
        "    \"Description\": d.description, \"Score\":BM25(d)\n",
        "    }\"\"\"\n",
        ")\n",
        "for doc in cursor:\n",
        "  print('\\033[4mTitle: ' + doc['Title'] + '\\033[0m')\n",
        "  print('\\033[4mDescription:\\033[0m ',textwrap.fill(doc['Description'], 90))\n",
        "  print('\\033[4mScore:\\033[0m ',str(doc['Score']))\n",
        "  print(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWpH7oVb8KnW"
      },
      "source": [
        "# Further Links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP0KpLrK8KnW"
      },
      "source": [
        "Hopefully, you can now see the potential that fuzzy search has with ArangoSearch. If you would like to continue learning more about ArangoDB and ArangoSearch here are some great next steps to get you started!\n",
        "\n",
        "* ArangoSearch Demo on Oasis (Just follow the onboarding guide)\n",
        "  * https://cloud.arangodb.com\n",
        "\n",
        "* ArangoSearch Documentation\n",
        "  * https://www.arangodb.com/docs/stable/arangosearch.html\n",
        "\n",
        "* ArangoSearch Training Center\n",
        "  * https://www.arangodb.com/arangodb-training-center/search/arangosearch/"
      ]
    }
  ]
}